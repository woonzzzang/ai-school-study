{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어별 벡터값 이미 임베딩 (어디에 기준이 되는 게 아니라 단어 독자적인 벡터값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가져온 게임 데이터 개수: 1000\n",
      "가져온 게임 데이터 개수: 2000\n",
      "가져온 게임 데이터 개수: 3000\n",
      "가져온 게임 데이터 개수: 4000\n",
      "가져온 게임 데이터 개수: 5000\n",
      "가져온 게임 데이터 개수: 6000\n",
      "가져온 게임 데이터 개수: 7000\n",
      "가져온 게임 데이터 개수: 8000\n",
      "가져온 게임 데이터 개수: 9000\n",
      "가져온 게임 데이터 개수: 10000\n",
      "가져온 게임 데이터 개수: 11000\n",
      "가져온 게임 데이터 개수: 12000\n",
      "가져온 게임 데이터 개수: 13000\n",
      "가져온 게임 데이터 개수: 14000\n",
      "가져온 게임 데이터 개수: 15000\n",
      "가져온 게임 데이터 개수: 16000\n",
      "가져온 게임 데이터 개수: 17000\n",
      "가져온 게임 데이터 개수: 18000\n",
      "가져온 게임 데이터 개수: 19000\n",
      "가져온 게임 데이터 개수: 20000\n",
      "가져온 게임 데이터 개수: 21000\n",
      "가져온 게임 데이터 개수: 22000\n",
      "가져온 게임 데이터 개수: 23000\n",
      "가져온 게임 데이터 개수: 24000\n",
      "가져온 게임 데이터 개수: 25000\n",
      "가져온 게임 데이터 개수: 26000\n",
      "가져온 게임 데이터 개수: 26249\n",
      "전처리된 문장 수: 24893\n",
      "훈련 시간: 7.60초\n",
      "Word2Vec 모델이 models\\word2vec_model.bin에 저장되었습니다.\n",
      "가져온 게임 데이터 개수: 1000\n",
      "가져온 게임 데이터 개수: 2000\n",
      "가져온 게임 데이터 개수: 3000\n",
      "가져온 게임 데이터 개수: 4000\n",
      "가져온 게임 데이터 개수: 5000\n",
      "가져온 게임 데이터 개수: 6000\n",
      "가져온 게임 데이터 개수: 7000\n",
      "가져온 게임 데이터 개수: 8000\n",
      "가져온 게임 데이터 개수: 9000\n",
      "가져온 게임 데이터 개수: 10000\n",
      "가져온 게임 데이터 개수: 11000\n",
      "가져온 게임 데이터 개수: 12000\n",
      "가져온 게임 데이터 개수: 13000\n",
      "가져온 게임 데이터 개수: 14000\n",
      "가져온 게임 데이터 개수: 15000\n",
      "가져온 게임 데이터 개수: 16000\n",
      "가져온 게임 데이터 개수: 17000\n",
      "가져온 게임 데이터 개수: 18000\n",
      "가져온 게임 데이터 개수: 19000\n",
      "가져온 게임 데이터 개수: 20000\n",
      "가져온 게임 데이터 개수: 21000\n",
      "가져온 게임 데이터 개수: 22000\n",
      "가져온 게임 데이터 개수: 23000\n",
      "가져온 게임 데이터 개수: 24000\n",
      "가져온 게임 데이터 개수: 25000\n",
      "가져온 게임 데이터 개수: 26000\n",
      "가져온 게임 데이터 개수: 26249\n",
      "총 가져온 게임 데이터 개수: 26249\n",
      "임베딩 데이터가 models\\game_embeddings.pkl에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Stopwords 다운로드\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Supabase 클라이언트 설정\n",
    "url = \"https://nhcmippskpgkykwsumqp.supabase.co\"\n",
    "key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5oY21pcHBza3Bna3lrd3N1bXFwIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MjE2MjYyNzEsImV4cCI6MjAzNzIwMjI3MX0.quApu8EwzqcTgcxdWezDvpZIHSX9LKVQ_NytpLBeAiY\"\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = []\n",
    "    if isinstance(text, list):\n",
    "        for phrase in text:\n",
    "            words.extend([word.lower() for word in nltk.word_tokenize(phrase) if word.isalnum() and word not in stop_words])\n",
    "    else:\n",
    "        words = [word.lower() for word in nltk.word_tokenize(text) if word.isalnum() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Word2Vec 모델 훈련 및 저장 함수\n",
    "def train_and_save_word2vec(games, model_path):\n",
    "    sentences = [preprocess_text(game['description_phrases']) for game in games if game['description_phrases']]\n",
    "    sentences = [sentence for sentence in sentences if sentence]  # 빈 문장 제거\n",
    "    print(f\"전처리된 문장 수: {len(sentences)}\")  # 디버깅용 출력\n",
    "    start_time = time.time()\n",
    "    model = Word2Vec(sentences, vector_size=150, window=7, min_count=2, workers=4, epochs=10, seed=42)\n",
    "    end_time = time.time()\n",
    "    print(f\"훈련 시간: {end_time - start_time:.2f}초\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Word2Vec 모델이 {model_path}에 저장되었습니다.\")\n",
    "\n",
    "# 페이지네이션을 사용하여 모든 데이터를 가져오는 함수\n",
    "def fetch_all_games():\n",
    "    limit = 1000\n",
    "    offset = 0\n",
    "    all_games = []\n",
    "    \n",
    "    while True:\n",
    "        response = supabase.table('steamsearcher_duplicate').select('appid, name, genre, recommendation_count, description_phrases').range(offset, offset + limit - 1).execute()\n",
    "        games = response.data\n",
    "        if not games:\n",
    "            break\n",
    "        all_games.extend(games)\n",
    "        offset += limit\n",
    "        print(f\"가져온 게임 데이터 개수: {len(all_games)}\")  # 진행 상황 출력\n",
    "    \n",
    "    return all_games\n",
    "\n",
    "# 게임 데이터를 미리 임베딩하고 저장하는 함수\n",
    "def embed_and_save_game_data(model_path, embed_path):\n",
    "    games = fetch_all_games()\n",
    "    print(f\"총 가져온 게임 데이터 개수: {len(games)}\")\n",
    "    \n",
    "    model = Word2Vec.load(model_path)\n",
    "    embeddings = []\n",
    "    \n",
    "    for game in games:\n",
    "        if not game['description_phrases']:\n",
    "            continue\n",
    "        game_words = preprocess_text(game['description_phrases'])\n",
    "        game_vectors = [model.wv[word] for word in game_words if word in model.wv]\n",
    "        if not game_vectors:\n",
    "            continue\n",
    "        game_embedding = np.mean(game_vectors, axis=0)\n",
    "        embeddings.append({\n",
    "            'appid': game['appid'],\n",
    "            'name': game['name'],\n",
    "            'genre': game['genre'],\n",
    "            'recommendation_count': game['recommendation_count'],\n",
    "            'embedding': game_embedding,\n",
    "            'embedding_words': game_words\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(embeddings)\n",
    "    df.to_pickle(embed_path)\n",
    "    print(f\"임베딩 데이터가 {embed_path}에 저장되었습니다.\")\n",
    "\n",
    "# 임베딩 저장 함수 호출\n",
    "if __name__ == \"__main__\":\n",
    "    model_dir = \"models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, 'word2vec_model.bin')\n",
    "    \n",
    "    embed_path = os.path.join(model_dir, 'game_embeddings.pkl')\n",
    "    \n",
    "    # 모델 학습 및 저장\n",
    "    games = fetch_all_games()\n",
    "    train_and_save_word2vec(games, model_path)\n",
    "    \n",
    "    # 게임 데이터 임베딩 및 저장\n",
    "    embed_and_save_game_data(model_path, embed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.0)\n",
      "Downloading faiss_cpu-1.8.0.post1-cp312-cp312-win_amd64.whl (14.6 MB)\n",
      "   ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/14.6 MB 7.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.8/14.6 MB 10.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.9/14.6 MB 15.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.9/14.6 MB 17.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.8/14.6 MB 17.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.9/14.6 MB 18.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.0/14.6 MB 19.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.1/14.6 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 8.1/14.6 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 8.2/14.6 MB 20.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.3/14.6 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.3/14.6 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.3/14.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.5/14.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.6/14.6 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.6/14.6 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.6/14.6 MB 19.8 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/downtown/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/downtown/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어: battle\n",
      "Name: Battlefield™ 2042, Genre: Action, Adventure, Casual, Recommendation Count: 208302\n",
      "Common Words: battlefield\n",
      "Name: Titanfall® 2, Genre: Action, Recommendation Count: 197493\n",
      "Common Words: blast, campaign\n",
      "Name: Battlefield™ V, Genre: Action, Recommendation Count: 189366\n",
      "Common Words: battlefield, battles\n",
      "Name: Battlefield™ 1, Genre: Action, Massively Multiplayer, Recommendation Count: 132762\n",
      "Common Words: battlefield, battles\n",
      "Name: STAR WARS Jedi: Fallen Order™, Genre: Action, Adventure, Recommendation Count: 126185\n",
      "Common Words: battles\n",
      "Name: ULTRAKILL, Genre: Action, Indie, Early Access, Recommendation Count: 107448\n",
      "Common Words: attacks, campaign\n",
      "Name: MORDHAU, Genre: Action, Indie, Recommendation Count: 90396\n",
      "Common Words: battlefield, attacks, brutal\n",
      "Name: FOR HONOR™, Genre: Action, Recommendation Count: 89739\n",
      "Common Words: battles\n",
      "Name: Ravenfield, Genre: Action, Indie, Early Access, Recommendation Count: 65171\n",
      "Common Words: battles\n",
      "Name: METAL GEAR SOLID V: THE PHANTOM PAIN, Genre: Action, Adventure, Recommendation Count: 61467\n",
      "Common Words: battlefield\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Stopwords 다운로드\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = []\n",
    "    if isinstance(text, list):\n",
    "        for phrase in text:\n",
    "            words.extend([word.lower() for word in nltk.word_tokenize(phrase) if word.isalnum() and word not in stop_words])\n",
    "    else:\n",
    "        words = [word.lower() for word in nltk.word_tokenize(text) if word.isalnum() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# 검색어와 유사한 단어를 포함하는 게임을 찾는 함수\n",
    "def search_games(query, model, embeddings_df):\n",
    "    query_words = preprocess_text(query)\n",
    "    if not query_words:\n",
    "        print(\"검색어가 너무 짧습니다.\")\n",
    "        return []\n",
    "\n",
    "    similar_words = []\n",
    "    for word in query_words:\n",
    "        if word in model.wv:\n",
    "            similar_words.extend([w for w, _ in model.wv.most_similar(word, topn=10)])\n",
    "    \n",
    "    similar_words = set(similar_words)  # 유사한 단어들을 집합으로 만듭니다.\n",
    "\n",
    "    # 유사한 단어를 포함하는 게임 찾기\n",
    "    results = []\n",
    "    for _, row in embeddings_df.iterrows():\n",
    "        game_words = set(row['embedding_words'])\n",
    "        common_words = game_words.intersection(similar_words)\n",
    "        if common_words:\n",
    "            results.append({\n",
    "                'name': row['name'],\n",
    "                'genre': row['genre'],\n",
    "                'recommendation_count': row['recommendation_count'],\n",
    "                'common_words': common_words\n",
    "            })\n",
    "\n",
    "    results.sort(key=lambda x: -x['recommendation_count'])\n",
    "\n",
    "    return results[:10]\n",
    "\n",
    "# 검색어 입력 및 결과 출력\n",
    "def main_search():\n",
    "    query = input(\"검색어를 입력하세요: \")\n",
    "\n",
    "    # 모델 로드 경로 설정\n",
    "    model_dir = \"models\"\n",
    "    model_path = os.path.join(model_dir, 'word2vec_model.bin')\n",
    "    embed_path = os.path.join(model_dir, 'game_embeddings.pkl')\n",
    "    \n",
    "    model = Word2Vec.load(model_path)\n",
    "    embeddings_df = pd.read_pickle(embed_path)\n",
    "\n",
    "    top_games = search_games(query, model, embeddings_df)\n",
    "\n",
    "    print(f\"검색어: {query}\")\n",
    "    for game in top_games:\n",
    "        print(f\"Name: {game['name']}, Genre: {game['genre']}, Recommendation Count: {game['recommendation_count']}\")\n",
    "        print(f\"Common Words: {', '.join(game['common_words'])}\")\n",
    "\n",
    "# 검색 함수 호출\n",
    "if __name__ == \"__main__\":\n",
    "    main_search()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
