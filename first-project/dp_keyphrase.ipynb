{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pke\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# 필수 구성 요소 초기화\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Supabase 클라이언트 초기화\n",
    "url = 'https://nhcmippskpgkykwsumqp.supabase.co'\n",
    "key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5oY21pcHBza3Bna3lrd3N1bXFwIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MjE2MjYyNzEsImV4cCI6MjAzNzIwMjI3MX0.quApu8EwzqcTgcxdWezDvpZIHSX9LKVQ_NytpLBeAiY' \n",
    "supabase: Client = create_client(url, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   8%|▊         | 82/1000 [01:01<11:13,  1.36it/s]"
     ]
    }
   ],
   "source": [
    "# 데이터베이스에서 데이터 가져오기\n",
    "def fetch_data(batch_size=1000):\n",
    "    response = supabase.table('steamsearcher_duplicate')\\\n",
    "                       .select('appid', 'detailed_description')\\\n",
    "                       .is_('dp', None)\\\n",
    "                       .neq('detailed_description', None)\\\n",
    "                       .limit(batch_size)\\\n",
    "                       .execute()\n",
    "    data = response.data\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# HTML 태그와 이미지의 src 속성 제거 함수\n",
    "def clean_html(raw_html):\n",
    "    if raw_html is None:\n",
    "        return None\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(r'http\\S+', '', text)  # 링크 제거\n",
    "    text = re.sub(r'\\s*src=\"[^\"]*\"', '', text)  # src 속성 제거\n",
    "    return text if text.strip() else None\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.lower() not in stop_words and not word.isdigit()]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# 키프레이즈 추출 함수\n",
    "def extract_keyphrases(text):\n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "    extractor.load_document(input=text, language='en')\n",
    "    extractor.candidate_selection(pos={'NOUN', 'PROPN', 'ADJ'})\n",
    "    extractor.candidate_weighting()\n",
    "    keyphrases = extractor.get_n_best(n=5)  # 원하는 키프레이즈 개수 설정\n",
    "    return ', '.join([keyphrase[0] for keyphrase in keyphrases])\n",
    "\n",
    "# 한 행씩 데이터 처리 및 업데이트\n",
    "def process_and_update_row(row):\n",
    "    cleaned_text = clean_html(row['detailed_description'])\n",
    "    if cleaned_text is None:\n",
    "        dp = None\n",
    "    else:\n",
    "        preprocessed_text = preprocess_text(cleaned_text)\n",
    "        dp = extract_keyphrases(preprocessed_text)\n",
    "    supabase.table('steamsearcher_duplicate').update({'dp': dp}).eq('appid', row['appid']).execute()\n",
    "\n",
    "# 메인 함수\n",
    "def main():\n",
    "    while True:\n",
    "        df = fetch_data()\n",
    "        if df.empty:\n",
    "            print(\"No more data to process\")\n",
    "            break\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "            try:\n",
    "                process_and_update_row(row)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing appid {row['appid']}: {e}\")\n",
    "        print(\"Batch processed. Fetching next batch...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
