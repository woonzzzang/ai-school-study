{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/downtown/miniconda3/envs/aug_env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "data_dir = \"/home/downtown/new_folder/deep-learning/module/module-1/chest_xray\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# 데이터프레임 생성 함수 정의\n",
    "def create_dataframe(data_dir):\n",
    "    image_paths = glob.glob(f\"{data_dir}/*/*\")\n",
    "    data = {'image_path': [], 'label': []}\n",
    "    for path in image_paths:\n",
    "        if 'NORMAL' in path:\n",
    "            data['image_path'].append(path)\n",
    "            data['label'].append(0)  # NORMAL -> 0\n",
    "        elif 'PNEUMONIA' in path:\n",
    "            data['image_path'].append(path)\n",
    "            data['label'].append(1)  # PNEUMONIA -> 1\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Train과 Test 데이터프레임 생성\n",
    "train_df = create_dataframe(train_dir)\n",
    "test_df = create_dataframe(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 이미지 파일 수: 5216\n",
      "Test 이미지 파일 수: 640\n"
     ]
    }
   ],
   "source": [
    "# 이미지 파일 경로 확인\n",
    "print(\"Train 이미지 파일 수:\", len(glob.glob(f\"{train_dir}/*/*\")))\n",
    "print(\"Test 이미지 파일 수:\", len(glob.glob(f\"{test_dir}/*/*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx, 0]\n",
    "        image = read_image(img_path)  # 이미지 로드 (0-255 사이의 값, uint8 형식)\n",
    "        label = self.df.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # 변환 적용\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 이미지의 텐서 shape: torch.Size([1, 1233, 1596])\n"
     ]
    }
   ],
   "source": [
    "# Dataset 인스턴스 생성\n",
    "dataset = ImageDataset(train_df)\n",
    "\n",
    "# 첫 번째 이미지의 텐서 shape 읽기\n",
    "image_tensor, _ = dataset[0]  # 데이터셋의 첫 번째 항목을 불러옴\n",
    "print(f\"첫 번째 이미지의 텐서 shape: {image_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 및 데이터 로더 생성\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), \n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ConvertImageDtype(torch.float32),  \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 흑백 이미지의 평균과 표준편차로 정규화\n",
    "])\n",
    "\n",
    "# 전체 학습 데이터셋 생성\n",
    "full_train_dataset = ImageDataset(train_df, transform=transform)\n",
    "\n",
    "# 학습 데이터셋과 검증 데이터셋으로 나누기 (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_dataset = ImageDataset(test_df, transform=transform)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.20298243063779278, Validation Loss: 0.3269733734654658\n",
      "Epoch 2, Train Loss: 0.13299307006745392, Validation Loss: 0.12622957123499928\n",
      "Epoch 3, Train Loss: 0.10403445456887929, Validation Loss: 0.09919128945153771\n",
      "Epoch 4, Train Loss: 0.09094876746597294, Validation Loss: 0.09144820820427302\n",
      "Epoch 5, Train Loss: 0.07405285625398614, Validation Loss: 0.10049939130178907\n",
      "Epoch 6, Train Loss: 0.06811378601554582, Validation Loss: 0.10263834443564217\n",
      "Epoch 7, Train Loss: 0.05098296432108938, Validation Loss: 0.12088930544753869\n",
      "Epoch 8, Train Loss: 0.05414596599800413, Validation Loss: 0.16770018290051006\n",
      "Epoch 9, Train Loss: 0.048860807483660354, Validation Loss: 0.10821807542533586\n",
      "Epoch 10, Train Loss: 0.039668656118704675, Validation Loss: 0.12876197989945384\n",
      "Epoch 11, Train Loss: 0.03124942039566129, Validation Loss: 0.12849632738101663\n",
      "Epoch 12, Train Loss: 0.03900818893142073, Validation Loss: 0.15138642003787964\n",
      "Epoch 13, Train Loss: 0.039618083264079756, Validation Loss: 0.12923864591302295\n",
      "Epoch 14, Train Loss: 0.041062952523148184, Validation Loss: 0.1384273126975379\n",
      "Epoch 15, Train Loss: 0.022795200717585687, Validation Loss: 0.10888850192228954\n",
      "Epoch 16, Train Loss: 0.021122739231302794, Validation Loss: 0.12197194087573072\n",
      "Epoch 17, Train Loss: 0.019151590912152287, Validation Loss: 0.14660349010690962\n",
      "Epoch 18, Train Loss: 0.03516436085117273, Validation Loss: 0.1656833183951676\n",
      "Epoch 19, Train Loss: 0.04290309288198633, Validation Loss: 0.11884832703931765\n",
      "Epoch 20, Train Loss: 0.015553183639457605, Validation Loss: 0.14733475267403814\n"
     ]
    }
   ],
   "source": [
    "# 이진 분류 모델 정의\n",
    "class BinaryClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassificationModel, self).__init__()\n",
    "        self.layer_1 = nn.Linear(256 * 256, 128) \n",
    "        self.bn1 = nn.BatchNorm1d(128)  \n",
    "        self.layer_2 = nn.Linear(128, 64) \n",
    "        self.bn2 = nn.BatchNorm1d(64) \n",
    "        self.layer_3 = nn.Linear(64, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.bn1(x) \n",
    "        x = torch.relu(x)  \n",
    "        x = self.layer_2(x)\n",
    "        x = self.bn2(x)  \n",
    "        x = torch.relu(x)  \n",
    "        z = self.layer_3(x)\n",
    "        return z\n",
    "\n",
    "model = BinaryClassificationModel()\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 모델 학습 루프\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)  # labels 모양을 (batch, 1)로 맞춤\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # 검증 단계\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {epoch_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.1062989481584737\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가 및 예측\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)  # 테스트 손실 계산\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()  # 0.5 기준으로 이진 분류\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy().flatten())\n",
    "\n",
    "# 최종 테스트 손실 출력\n",
    "print(f\"Test Loss: {test_loss / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       0.95      0.35      0.51       242\n",
      "   PNEUMONIA       0.71      0.99      0.83       398\n",
      "\n",
      "    accuracy                           0.75       640\n",
      "   macro avg       0.83      0.67      0.67       640\n",
      "weighted avg       0.80      0.75      0.71       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 분류 레포트 출력\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(all_labels, all_predictions, target_names=class_labels)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aug_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
