{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # True 이면 GPU가 잡혀있는 것\n",
    "\n",
    "# Mac은 torch.backends.mps.is_available()\n",
    "# tensorflow 와는 다르게 GPU를 우리가 지정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 파이토치 텐서 선언하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.]) # numpy array의 자식 class라서 메서드 사용 가능\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(t.dim()) # rank (순위를 표현하기도 하지만 행렬의 복잡도를 표현하기도 한다. 높을 수록 복잡)\n",
    "# dim : 차원, rank라고 하기도 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(1.) tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "print(t[0], t[1], t[-1]) # Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 4.]) tensor([4., 5.])\n"
     ]
    }
   ],
   "source": [
    "print(t[2:5], t[4:-1]) # Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2차원 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim()) # rank\n",
    "print(t.size()) # shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[2, 1] # 행을 먼저 지정하고 열을 지정하면 특정 값을 가져올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7., 8., 9.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  6.,  9., 12.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브로드 캐스팅 pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 최대(Max)와 아그맥스(ArgMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([3., 4.]),\n",
       "indices=tensor([1, 1]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim=0) # 차원 하나 제거 후 나머지 애들을 표현, 덩어리 채로 , 행의 인덱스가 반환 되는 것 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'horse'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10개의 사물 분류 모델\n",
    "result = torch.FloatTensor([0.1, 0.04, 0.5, 0.1, 0.1, 0.01, 0.01, 0.01, 0.87, 0.1])\n",
    "names = ['cat', 'dog', 'lion', 'tiger', 'worf', 'fox', 'snake', 'rabbit', 'horse', 'zebra']\n",
    "result.argmax()\n",
    "names[result.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 뷰(View) - 원소의 수를 유지하면서 텐서의 크기 변경. 매우 중요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ft.view(list 형식의 차원 shape)\n",
    "reshaped_ft = ft.view([3, 4])\n",
    "reshaped_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_ft = ft.view([4, 3])\n",
    "reshaped_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_ft = ft.view([3, -1]) # -1 : 나는 모르겠으니까 알아서 잡아달라는 뜻\n",
    "reshaped_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[5, -1]' is invalid for input of size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reshaped_ft \u001b[38;5;241m=\u001b[39m \u001b[43mft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m reshaped_ft\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 5는 12를 깔끔하게 나누어 떨어지지 않기 때문에 안 됨\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[5, -1]' is invalid for input of size 12"
     ]
    }
   ],
   "source": [
    "reshaped_ft = ft.view([5, -1])\n",
    "reshaped_ft.shape\n",
    "\n",
    "# 5로는 12를 깔끔하게 나누어 떨어지지 않기 때문에 안 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_ft = ft.view([4, 3, 1])\n",
    "reshaped_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_ft = ft.view([2, 2, 1, 3]) # 숫자만 맞으면 4차원도 가능\n",
    "reshaped_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.ones([128, 128])\n",
    "tf = torch.FloatTensor(t)\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = tf.view([128, 128, 1]) # 행렬 숫자의 차원을 3차원으로 맞춰준다.\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 차원 하나를 제거해주는 것 : 스퀴즈\n",
    "\n",
    "tf = tf.squeeze()\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = tf.view([-1, 1]) # 앞은 얼마인지 모르지만 뒤는 1로 맞춰줘라\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = tf.squeeze()\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16384])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0번 째 인덱스에 하나의 차원을 추가하고 싶다면?\n",
    "\n",
    "tf = tf.unsqueeze(0) # 자유자재로 사용할 수 있어야 한다. 원하는 자리에 차원 1을 추가해줌ㄴ\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이토치로 선형회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = wx + b 에서 w, b를 알고 싶은 것, 그러기 위해서는 별도의 특수한 공간에 넣어주어야 한다. \n",
    "# 이런 파라미터의 특징은 값이 학습을 통해서 변화한다.\n",
    "# 바뀌는 행렬이라는 점을 선언해줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 그냥 torch.zeros는 그냥 일반적인 0으로 채워지는 행렬이 만들어지지만 \n",
    "# requires_grad=True 를 하면 학습을 통해 역전파 이후 값이 바뀐다.\n",
    "# 파라미터 값들은 requires_grad=True를 반드시 켜줘야 한다 !!! 중요\n",
    "# 가중치가 학습 가능하다는 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "현재 식의 상태\n",
    "y = 0 * x + 0\n",
    "'''\n",
    "\n",
    "y_pred = x_train * W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred\n",
    "# grad_fn=<AddBackward0>\n",
    "# 이 값은 기울기 계산을 통해서 조정할 수 있다?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.mean((y_train - y_pred) ** 2) # MSE : 평균 제곱 손실\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.001) # optim 을 불러오고 적용 대상과 학습률 설정\n",
    "# optim.SGD(model.parameters(), lr=0.001) : 일종의 compile 단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 바퀴를 돌릴 때 마다 grad를 초기화 해야 한다.\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 우리가 손실값을 만들었을 때 requires_grad=True를 넣었으면 아래 태그를 넣어줘야 한다. 학습 반영하도록 !\n",
    "\n",
    "loss.backward() \n",
    "\n",
    "optimizer.step() # : 파라미터 업데이트\n",
    "\n",
    "# -> 손실 계산 후 역전파 하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch    1/20 W: 0.353, b: 0.151 Cost: 14.770963\n",
      "Epoch    2/20 W: 0.500, b: 0.214 Cost: 11.691541\n",
      "Epoch    3/20 W: 0.632, b: 0.270 Cost: 9.257346\n",
      "Epoch    4/20 W: 0.749, b: 0.319 Cost: 7.333169\n",
      "Epoch    5/20 W: 0.853, b: 0.363 Cost: 5.812135\n",
      "Epoch    6/20 W: 0.945, b: 0.401 Cost: 4.609764\n",
      "Epoch    7/20 W: 1.028, b: 0.435 Cost: 3.659278\n",
      "Epoch    8/20 W: 1.101, b: 0.466 Cost: 2.907896\n",
      "Epoch    9/20 W: 1.166, b: 0.492 Cost: 2.313895\n",
      "Epoch   10/20 W: 1.224, b: 0.516 Cost: 1.844294\n",
      "Epoch   11/20 W: 1.276, b: 0.536 Cost: 1.473027\n",
      "Epoch   12/20 W: 1.322, b: 0.555 Cost: 1.179487\n",
      "Epoch   13/20 W: 1.363, b: 0.571 Cost: 0.947386\n",
      "Epoch   14/20 W: 1.400, b: 0.585 Cost: 0.763851\n",
      "Epoch   15/20 W: 1.433, b: 0.597 Cost: 0.618704\n",
      "Epoch   16/20 W: 1.462, b: 0.608 Cost: 0.503902\n",
      "Epoch   17/20 W: 1.488, b: 0.617 Cost: 0.413086\n",
      "Epoch   18/20 W: 1.511, b: 0.625 Cost: 0.341229\n",
      "Epoch   19/20 W: 1.531, b: 0.632 Cost: 0.284360\n",
      "Epoch   20/20 W: 1.550, b: 0.638 Cost: 0.239337\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 20 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    # if epoch % 100 == 0:\n",
    "    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 다중 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 여기에 들어가는 가중치의 개수는 3개 : x1, x2, x3 로 구성되어 있기 때문\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3466,  0.0233, -0.4087]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0657], requires_grad=True)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Linear(3, 1) # parameter = input * output + 1 (+1은 bias)\n",
    "# model.parameters() # 여기까지만 하면 <generator object Module.parameters at 0x7fb2a029af80> 이렇게 뜨는데 generator니까 list에 담아줘야 됨.\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/500 Cost: 30929.994141\n",
      "Epoch    1/500 Cost: 5964451840.000000\n",
      "Epoch    2/500 Cost: 1150190899691520.000000\n",
      "Epoch    3/500 Cost: 221804040866601369600.000000\n",
      "Epoch    4/500 Cost: 42772926652312180070809600.000000\n",
      "Epoch    5/500 Cost: 8248376952334104359529831464960.000000\n",
      "Epoch    6/500 Cost: 1590625789280878258983536989799710720.000000\n",
      "Epoch    7/500 Cost: inf\n",
      "Epoch    8/500 Cost: inf\n",
      "Epoch    9/500 Cost: inf\n",
      "Epoch   10/500 Cost: inf\n",
      "Epoch   11/500 Cost: inf\n",
      "Epoch   12/500 Cost: inf\n",
      "Epoch   13/500 Cost: inf\n",
      "Epoch   14/500 Cost: inf\n",
      "Epoch   15/500 Cost: nan\n",
      "Epoch   16/500 Cost: nan\n",
      "Epoch   17/500 Cost: nan\n",
      "Epoch   18/500 Cost: nan\n",
      "Epoch   19/500 Cost: nan\n",
      "Epoch   20/500 Cost: nan\n",
      "Epoch   21/500 Cost: nan\n",
      "Epoch   22/500 Cost: nan\n",
      "Epoch   23/500 Cost: nan\n",
      "Epoch   24/500 Cost: nan\n",
      "Epoch   25/500 Cost: nan\n",
      "Epoch   26/500 Cost: nan\n",
      "Epoch   27/500 Cost: nan\n",
      "Epoch   28/500 Cost: nan\n",
      "Epoch   29/500 Cost: nan\n",
      "Epoch   30/500 Cost: nan\n",
      "Epoch   31/500 Cost: nan\n",
      "Epoch   32/500 Cost: nan\n",
      "Epoch   33/500 Cost: nan\n",
      "Epoch   34/500 Cost: nan\n",
      "Epoch   35/500 Cost: nan\n",
      "Epoch   36/500 Cost: nan\n",
      "Epoch   37/500 Cost: nan\n",
      "Epoch   38/500 Cost: nan\n",
      "Epoch   39/500 Cost: nan\n",
      "Epoch   40/500 Cost: nan\n",
      "Epoch   41/500 Cost: nan\n",
      "Epoch   42/500 Cost: nan\n",
      "Epoch   43/500 Cost: nan\n",
      "Epoch   44/500 Cost: nan\n",
      "Epoch   45/500 Cost: nan\n",
      "Epoch   46/500 Cost: nan\n",
      "Epoch   47/500 Cost: nan\n",
      "Epoch   48/500 Cost: nan\n",
      "Epoch   49/500 Cost: nan\n",
      "Epoch   50/500 Cost: nan\n",
      "Epoch   51/500 Cost: nan\n",
      "Epoch   52/500 Cost: nan\n",
      "Epoch   53/500 Cost: nan\n",
      "Epoch   54/500 Cost: nan\n",
      "Epoch   55/500 Cost: nan\n",
      "Epoch   56/500 Cost: nan\n",
      "Epoch   57/500 Cost: nan\n",
      "Epoch   58/500 Cost: nan\n",
      "Epoch   59/500 Cost: nan\n",
      "Epoch   60/500 Cost: nan\n",
      "Epoch   61/500 Cost: nan\n",
      "Epoch   62/500 Cost: nan\n",
      "Epoch   63/500 Cost: nan\n",
      "Epoch   64/500 Cost: nan\n",
      "Epoch   65/500 Cost: nan\n",
      "Epoch   66/500 Cost: nan\n",
      "Epoch   67/500 Cost: nan\n",
      "Epoch   68/500 Cost: nan\n",
      "Epoch   69/500 Cost: nan\n",
      "Epoch   70/500 Cost: nan\n",
      "Epoch   71/500 Cost: nan\n",
      "Epoch   72/500 Cost: nan\n",
      "Epoch   73/500 Cost: nan\n",
      "Epoch   74/500 Cost: nan\n",
      "Epoch   75/500 Cost: nan\n",
      "Epoch   76/500 Cost: nan\n",
      "Epoch   77/500 Cost: nan\n",
      "Epoch   78/500 Cost: nan\n",
      "Epoch   79/500 Cost: nan\n",
      "Epoch   80/500 Cost: nan\n",
      "Epoch   81/500 Cost: nan\n",
      "Epoch   82/500 Cost: nan\n",
      "Epoch   83/500 Cost: nan\n",
      "Epoch   84/500 Cost: nan\n",
      "Epoch   85/500 Cost: nan\n",
      "Epoch   86/500 Cost: nan\n",
      "Epoch   87/500 Cost: nan\n",
      "Epoch   88/500 Cost: nan\n",
      "Epoch   89/500 Cost: nan\n",
      "Epoch   90/500 Cost: nan\n",
      "Epoch   91/500 Cost: nan\n",
      "Epoch   92/500 Cost: nan\n",
      "Epoch   93/500 Cost: nan\n",
      "Epoch   94/500 Cost: nan\n",
      "Epoch   95/500 Cost: nan\n",
      "Epoch   96/500 Cost: nan\n",
      "Epoch   97/500 Cost: nan\n",
      "Epoch   98/500 Cost: nan\n",
      "Epoch   99/500 Cost: nan\n",
      "Epoch  100/500 Cost: nan\n",
      "Epoch  101/500 Cost: nan\n",
      "Epoch  102/500 Cost: nan\n",
      "Epoch  103/500 Cost: nan\n",
      "Epoch  104/500 Cost: nan\n",
      "Epoch  105/500 Cost: nan\n",
      "Epoch  106/500 Cost: nan\n",
      "Epoch  107/500 Cost: nan\n",
      "Epoch  108/500 Cost: nan\n",
      "Epoch  109/500 Cost: nan\n",
      "Epoch  110/500 Cost: nan\n",
      "Epoch  111/500 Cost: nan\n",
      "Epoch  112/500 Cost: nan\n",
      "Epoch  113/500 Cost: nan\n",
      "Epoch  114/500 Cost: nan\n",
      "Epoch  115/500 Cost: nan\n",
      "Epoch  116/500 Cost: nan\n",
      "Epoch  117/500 Cost: nan\n",
      "Epoch  118/500 Cost: nan\n",
      "Epoch  119/500 Cost: nan\n",
      "Epoch  120/500 Cost: nan\n",
      "Epoch  121/500 Cost: nan\n",
      "Epoch  122/500 Cost: nan\n",
      "Epoch  123/500 Cost: nan\n",
      "Epoch  124/500 Cost: nan\n",
      "Epoch  125/500 Cost: nan\n",
      "Epoch  126/500 Cost: nan\n",
      "Epoch  127/500 Cost: nan\n",
      "Epoch  128/500 Cost: nan\n",
      "Epoch  129/500 Cost: nan\n",
      "Epoch  130/500 Cost: nan\n",
      "Epoch  131/500 Cost: nan\n",
      "Epoch  132/500 Cost: nan\n",
      "Epoch  133/500 Cost: nan\n",
      "Epoch  134/500 Cost: nan\n",
      "Epoch  135/500 Cost: nan\n",
      "Epoch  136/500 Cost: nan\n",
      "Epoch  137/500 Cost: nan\n",
      "Epoch  138/500 Cost: nan\n",
      "Epoch  139/500 Cost: nan\n",
      "Epoch  140/500 Cost: nan\n",
      "Epoch  141/500 Cost: nan\n",
      "Epoch  142/500 Cost: nan\n",
      "Epoch  143/500 Cost: nan\n",
      "Epoch  144/500 Cost: nan\n",
      "Epoch  145/500 Cost: nan\n",
      "Epoch  146/500 Cost: nan\n",
      "Epoch  147/500 Cost: nan\n",
      "Epoch  148/500 Cost: nan\n",
      "Epoch  149/500 Cost: nan\n",
      "Epoch  150/500 Cost: nan\n",
      "Epoch  151/500 Cost: nan\n",
      "Epoch  152/500 Cost: nan\n",
      "Epoch  153/500 Cost: nan\n",
      "Epoch  154/500 Cost: nan\n",
      "Epoch  155/500 Cost: nan\n",
      "Epoch  156/500 Cost: nan\n",
      "Epoch  157/500 Cost: nan\n",
      "Epoch  158/500 Cost: nan\n",
      "Epoch  159/500 Cost: nan\n",
      "Epoch  160/500 Cost: nan\n",
      "Epoch  161/500 Cost: nan\n",
      "Epoch  162/500 Cost: nan\n",
      "Epoch  163/500 Cost: nan\n",
      "Epoch  164/500 Cost: nan\n",
      "Epoch  165/500 Cost: nan\n",
      "Epoch  166/500 Cost: nan\n",
      "Epoch  167/500 Cost: nan\n",
      "Epoch  168/500 Cost: nan\n",
      "Epoch  169/500 Cost: nan\n",
      "Epoch  170/500 Cost: nan\n",
      "Epoch  171/500 Cost: nan\n",
      "Epoch  172/500 Cost: nan\n",
      "Epoch  173/500 Cost: nan\n",
      "Epoch  174/500 Cost: nan\n",
      "Epoch  175/500 Cost: nan\n",
      "Epoch  176/500 Cost: nan\n",
      "Epoch  177/500 Cost: nan\n",
      "Epoch  178/500 Cost: nan\n",
      "Epoch  179/500 Cost: nan\n",
      "Epoch  180/500 Cost: nan\n",
      "Epoch  181/500 Cost: nan\n",
      "Epoch  182/500 Cost: nan\n",
      "Epoch  183/500 Cost: nan\n",
      "Epoch  184/500 Cost: nan\n",
      "Epoch  185/500 Cost: nan\n",
      "Epoch  186/500 Cost: nan\n",
      "Epoch  187/500 Cost: nan\n",
      "Epoch  188/500 Cost: nan\n",
      "Epoch  189/500 Cost: nan\n",
      "Epoch  190/500 Cost: nan\n",
      "Epoch  191/500 Cost: nan\n",
      "Epoch  192/500 Cost: nan\n",
      "Epoch  193/500 Cost: nan\n",
      "Epoch  194/500 Cost: nan\n",
      "Epoch  195/500 Cost: nan\n",
      "Epoch  196/500 Cost: nan\n",
      "Epoch  197/500 Cost: nan\n",
      "Epoch  198/500 Cost: nan\n",
      "Epoch  199/500 Cost: nan\n",
      "Epoch  200/500 Cost: nan\n",
      "Epoch  201/500 Cost: nan\n",
      "Epoch  202/500 Cost: nan\n",
      "Epoch  203/500 Cost: nan\n",
      "Epoch  204/500 Cost: nan\n",
      "Epoch  205/500 Cost: nan\n",
      "Epoch  206/500 Cost: nan\n",
      "Epoch  207/500 Cost: nan\n",
      "Epoch  208/500 Cost: nan\n",
      "Epoch  209/500 Cost: nan\n",
      "Epoch  210/500 Cost: nan\n",
      "Epoch  211/500 Cost: nan\n",
      "Epoch  212/500 Cost: nan\n",
      "Epoch  213/500 Cost: nan\n",
      "Epoch  214/500 Cost: nan\n",
      "Epoch  215/500 Cost: nan\n",
      "Epoch  216/500 Cost: nan\n",
      "Epoch  217/500 Cost: nan\n",
      "Epoch  218/500 Cost: nan\n",
      "Epoch  219/500 Cost: nan\n",
      "Epoch  220/500 Cost: nan\n",
      "Epoch  221/500 Cost: nan\n",
      "Epoch  222/500 Cost: nan\n",
      "Epoch  223/500 Cost: nan\n",
      "Epoch  224/500 Cost: nan\n",
      "Epoch  225/500 Cost: nan\n",
      "Epoch  226/500 Cost: nan\n",
      "Epoch  227/500 Cost: nan\n",
      "Epoch  228/500 Cost: nan\n",
      "Epoch  229/500 Cost: nan\n",
      "Epoch  230/500 Cost: nan\n",
      "Epoch  231/500 Cost: nan\n",
      "Epoch  232/500 Cost: nan\n",
      "Epoch  233/500 Cost: nan\n",
      "Epoch  234/500 Cost: nan\n",
      "Epoch  235/500 Cost: nan\n",
      "Epoch  236/500 Cost: nan\n",
      "Epoch  237/500 Cost: nan\n",
      "Epoch  238/500 Cost: nan\n",
      "Epoch  239/500 Cost: nan\n",
      "Epoch  240/500 Cost: nan\n",
      "Epoch  241/500 Cost: nan\n",
      "Epoch  242/500 Cost: nan\n",
      "Epoch  243/500 Cost: nan\n",
      "Epoch  244/500 Cost: nan\n",
      "Epoch  245/500 Cost: nan\n",
      "Epoch  246/500 Cost: nan\n",
      "Epoch  247/500 Cost: nan\n",
      "Epoch  248/500 Cost: nan\n",
      "Epoch  249/500 Cost: nan\n",
      "Epoch  250/500 Cost: nan\n",
      "Epoch  251/500 Cost: nan\n",
      "Epoch  252/500 Cost: nan\n",
      "Epoch  253/500 Cost: nan\n",
      "Epoch  254/500 Cost: nan\n",
      "Epoch  255/500 Cost: nan\n",
      "Epoch  256/500 Cost: nan\n",
      "Epoch  257/500 Cost: nan\n",
      "Epoch  258/500 Cost: nan\n",
      "Epoch  259/500 Cost: nan\n",
      "Epoch  260/500 Cost: nan\n",
      "Epoch  261/500 Cost: nan\n",
      "Epoch  262/500 Cost: nan\n",
      "Epoch  263/500 Cost: nan\n",
      "Epoch  264/500 Cost: nan\n",
      "Epoch  265/500 Cost: nan\n",
      "Epoch  266/500 Cost: nan\n",
      "Epoch  267/500 Cost: nan\n",
      "Epoch  268/500 Cost: nan\n",
      "Epoch  269/500 Cost: nan\n",
      "Epoch  270/500 Cost: nan\n",
      "Epoch  271/500 Cost: nan\n",
      "Epoch  272/500 Cost: nan\n",
      "Epoch  273/500 Cost: nan\n",
      "Epoch  274/500 Cost: nan\n",
      "Epoch  275/500 Cost: nan\n",
      "Epoch  276/500 Cost: nan\n",
      "Epoch  277/500 Cost: nan\n",
      "Epoch  278/500 Cost: nan\n",
      "Epoch  279/500 Cost: nan\n",
      "Epoch  280/500 Cost: nan\n",
      "Epoch  281/500 Cost: nan\n",
      "Epoch  282/500 Cost: nan\n",
      "Epoch  283/500 Cost: nan\n",
      "Epoch  284/500 Cost: nan\n",
      "Epoch  285/500 Cost: nan\n",
      "Epoch  286/500 Cost: nan\n",
      "Epoch  287/500 Cost: nan\n",
      "Epoch  288/500 Cost: nan\n",
      "Epoch  289/500 Cost: nan\n",
      "Epoch  290/500 Cost: nan\n",
      "Epoch  291/500 Cost: nan\n",
      "Epoch  292/500 Cost: nan\n",
      "Epoch  293/500 Cost: nan\n",
      "Epoch  294/500 Cost: nan\n",
      "Epoch  295/500 Cost: nan\n",
      "Epoch  296/500 Cost: nan\n",
      "Epoch  297/500 Cost: nan\n",
      "Epoch  298/500 Cost: nan\n",
      "Epoch  299/500 Cost: nan\n",
      "Epoch  300/500 Cost: nan\n",
      "Epoch  301/500 Cost: nan\n",
      "Epoch  302/500 Cost: nan\n",
      "Epoch  303/500 Cost: nan\n",
      "Epoch  304/500 Cost: nan\n",
      "Epoch  305/500 Cost: nan\n",
      "Epoch  306/500 Cost: nan\n",
      "Epoch  307/500 Cost: nan\n",
      "Epoch  308/500 Cost: nan\n",
      "Epoch  309/500 Cost: nan\n",
      "Epoch  310/500 Cost: nan\n",
      "Epoch  311/500 Cost: nan\n",
      "Epoch  312/500 Cost: nan\n",
      "Epoch  313/500 Cost: nan\n",
      "Epoch  314/500 Cost: nan\n",
      "Epoch  315/500 Cost: nan\n",
      "Epoch  316/500 Cost: nan\n",
      "Epoch  317/500 Cost: nan\n",
      "Epoch  318/500 Cost: nan\n",
      "Epoch  319/500 Cost: nan\n",
      "Epoch  320/500 Cost: nan\n",
      "Epoch  321/500 Cost: nan\n",
      "Epoch  322/500 Cost: nan\n",
      "Epoch  323/500 Cost: nan\n",
      "Epoch  324/500 Cost: nan\n",
      "Epoch  325/500 Cost: nan\n",
      "Epoch  326/500 Cost: nan\n",
      "Epoch  327/500 Cost: nan\n",
      "Epoch  328/500 Cost: nan\n",
      "Epoch  329/500 Cost: nan\n",
      "Epoch  330/500 Cost: nan\n",
      "Epoch  331/500 Cost: nan\n",
      "Epoch  332/500 Cost: nan\n",
      "Epoch  333/500 Cost: nan\n",
      "Epoch  334/500 Cost: nan\n",
      "Epoch  335/500 Cost: nan\n",
      "Epoch  336/500 Cost: nan\n",
      "Epoch  337/500 Cost: nan\n",
      "Epoch  338/500 Cost: nan\n",
      "Epoch  339/500 Cost: nan\n",
      "Epoch  340/500 Cost: nan\n",
      "Epoch  341/500 Cost: nan\n",
      "Epoch  342/500 Cost: nan\n",
      "Epoch  343/500 Cost: nan\n",
      "Epoch  344/500 Cost: nan\n",
      "Epoch  345/500 Cost: nan\n",
      "Epoch  346/500 Cost: nan\n",
      "Epoch  347/500 Cost: nan\n",
      "Epoch  348/500 Cost: nan\n",
      "Epoch  349/500 Cost: nan\n",
      "Epoch  350/500 Cost: nan\n",
      "Epoch  351/500 Cost: nan\n",
      "Epoch  352/500 Cost: nan\n",
      "Epoch  353/500 Cost: nan\n",
      "Epoch  354/500 Cost: nan\n",
      "Epoch  355/500 Cost: nan\n",
      "Epoch  356/500 Cost: nan\n",
      "Epoch  357/500 Cost: nan\n",
      "Epoch  358/500 Cost: nan\n",
      "Epoch  359/500 Cost: nan\n",
      "Epoch  360/500 Cost: nan\n",
      "Epoch  361/500 Cost: nan\n",
      "Epoch  362/500 Cost: nan\n",
      "Epoch  363/500 Cost: nan\n",
      "Epoch  364/500 Cost: nan\n",
      "Epoch  365/500 Cost: nan\n",
      "Epoch  366/500 Cost: nan\n",
      "Epoch  367/500 Cost: nan\n",
      "Epoch  368/500 Cost: nan\n",
      "Epoch  369/500 Cost: nan\n",
      "Epoch  370/500 Cost: nan\n",
      "Epoch  371/500 Cost: nan\n",
      "Epoch  372/500 Cost: nan\n",
      "Epoch  373/500 Cost: nan\n",
      "Epoch  374/500 Cost: nan\n",
      "Epoch  375/500 Cost: nan\n",
      "Epoch  376/500 Cost: nan\n",
      "Epoch  377/500 Cost: nan\n",
      "Epoch  378/500 Cost: nan\n",
      "Epoch  379/500 Cost: nan\n",
      "Epoch  380/500 Cost: nan\n",
      "Epoch  381/500 Cost: nan\n",
      "Epoch  382/500 Cost: nan\n",
      "Epoch  383/500 Cost: nan\n",
      "Epoch  384/500 Cost: nan\n",
      "Epoch  385/500 Cost: nan\n",
      "Epoch  386/500 Cost: nan\n",
      "Epoch  387/500 Cost: nan\n",
      "Epoch  388/500 Cost: nan\n",
      "Epoch  389/500 Cost: nan\n",
      "Epoch  390/500 Cost: nan\n",
      "Epoch  391/500 Cost: nan\n",
      "Epoch  392/500 Cost: nan\n",
      "Epoch  393/500 Cost: nan\n",
      "Epoch  394/500 Cost: nan\n",
      "Epoch  395/500 Cost: nan\n",
      "Epoch  396/500 Cost: nan\n",
      "Epoch  397/500 Cost: nan\n",
      "Epoch  398/500 Cost: nan\n",
      "Epoch  399/500 Cost: nan\n",
      "Epoch  400/500 Cost: nan\n",
      "Epoch  401/500 Cost: nan\n",
      "Epoch  402/500 Cost: nan\n",
      "Epoch  403/500 Cost: nan\n",
      "Epoch  404/500 Cost: nan\n",
      "Epoch  405/500 Cost: nan\n",
      "Epoch  406/500 Cost: nan\n",
      "Epoch  407/500 Cost: nan\n",
      "Epoch  408/500 Cost: nan\n",
      "Epoch  409/500 Cost: nan\n",
      "Epoch  410/500 Cost: nan\n",
      "Epoch  411/500 Cost: nan\n",
      "Epoch  412/500 Cost: nan\n",
      "Epoch  413/500 Cost: nan\n",
      "Epoch  414/500 Cost: nan\n",
      "Epoch  415/500 Cost: nan\n",
      "Epoch  416/500 Cost: nan\n",
      "Epoch  417/500 Cost: nan\n",
      "Epoch  418/500 Cost: nan\n",
      "Epoch  419/500 Cost: nan\n",
      "Epoch  420/500 Cost: nan\n",
      "Epoch  421/500 Cost: nan\n",
      "Epoch  422/500 Cost: nan\n",
      "Epoch  423/500 Cost: nan\n",
      "Epoch  424/500 Cost: nan\n",
      "Epoch  425/500 Cost: nan\n",
      "Epoch  426/500 Cost: nan\n",
      "Epoch  427/500 Cost: nan\n",
      "Epoch  428/500 Cost: nan\n",
      "Epoch  429/500 Cost: nan\n",
      "Epoch  430/500 Cost: nan\n",
      "Epoch  431/500 Cost: nan\n",
      "Epoch  432/500 Cost: nan\n",
      "Epoch  433/500 Cost: nan\n",
      "Epoch  434/500 Cost: nan\n",
      "Epoch  435/500 Cost: nan\n",
      "Epoch  436/500 Cost: nan\n",
      "Epoch  437/500 Cost: nan\n",
      "Epoch  438/500 Cost: nan\n",
      "Epoch  439/500 Cost: nan\n",
      "Epoch  440/500 Cost: nan\n",
      "Epoch  441/500 Cost: nan\n",
      "Epoch  442/500 Cost: nan\n",
      "Epoch  443/500 Cost: nan\n",
      "Epoch  444/500 Cost: nan\n",
      "Epoch  445/500 Cost: nan\n",
      "Epoch  446/500 Cost: nan\n",
      "Epoch  447/500 Cost: nan\n",
      "Epoch  448/500 Cost: nan\n",
      "Epoch  449/500 Cost: nan\n",
      "Epoch  450/500 Cost: nan\n",
      "Epoch  451/500 Cost: nan\n",
      "Epoch  452/500 Cost: nan\n",
      "Epoch  453/500 Cost: nan\n",
      "Epoch  454/500 Cost: nan\n",
      "Epoch  455/500 Cost: nan\n",
      "Epoch  456/500 Cost: nan\n",
      "Epoch  457/500 Cost: nan\n",
      "Epoch  458/500 Cost: nan\n",
      "Epoch  459/500 Cost: nan\n",
      "Epoch  460/500 Cost: nan\n",
      "Epoch  461/500 Cost: nan\n",
      "Epoch  462/500 Cost: nan\n",
      "Epoch  463/500 Cost: nan\n",
      "Epoch  464/500 Cost: nan\n",
      "Epoch  465/500 Cost: nan\n",
      "Epoch  466/500 Cost: nan\n",
      "Epoch  467/500 Cost: nan\n",
      "Epoch  468/500 Cost: nan\n",
      "Epoch  469/500 Cost: nan\n",
      "Epoch  470/500 Cost: nan\n",
      "Epoch  471/500 Cost: nan\n",
      "Epoch  472/500 Cost: nan\n",
      "Epoch  473/500 Cost: nan\n",
      "Epoch  474/500 Cost: nan\n",
      "Epoch  475/500 Cost: nan\n",
      "Epoch  476/500 Cost: nan\n",
      "Epoch  477/500 Cost: nan\n",
      "Epoch  478/500 Cost: nan\n",
      "Epoch  479/500 Cost: nan\n",
      "Epoch  480/500 Cost: nan\n",
      "Epoch  481/500 Cost: nan\n",
      "Epoch  482/500 Cost: nan\n",
      "Epoch  483/500 Cost: nan\n",
      "Epoch  484/500 Cost: nan\n",
      "Epoch  485/500 Cost: nan\n",
      "Epoch  486/500 Cost: nan\n",
      "Epoch  487/500 Cost: nan\n",
      "Epoch  488/500 Cost: nan\n",
      "Epoch  489/500 Cost: nan\n",
      "Epoch  490/500 Cost: nan\n",
      "Epoch  491/500 Cost: nan\n",
      "Epoch  492/500 Cost: nan\n",
      "Epoch  493/500 Cost: nan\n",
      "Epoch  494/500 Cost: nan\n",
      "Epoch  495/500 Cost: nan\n",
      "Epoch  496/500 Cost: nan\n",
      "Epoch  497/500 Cost: nan\n",
      "Epoch  498/500 Cost: nan\n",
      "Epoch  499/500 Cost: nan\n",
      "Epoch  500/500 Cost: nan\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train) # 순전파\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epochs % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[nan, nan, nan]], requires_grad=True), Parameter containing:\n",
      "tensor([nan], requires_grad=True)]\n",
      "tensor([[nan]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_x = torch.FloatTensor([[73, 80, 75]])\n",
    "new_y = model(new_x)\n",
    "\n",
    "print(list(model.parameters()))\n",
    "print(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. 상속을 받야야 됨. nn.Module\\n2. def __init__(우리가 쓰고 싶은 Linear의 이름을 쭉 써주면 된다.), def forward 반드시 넣어줘야 한다.\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class로 모델 만드는 법\n",
    "'''\n",
    "1. 상속을 받야야 됨. nn.Module\n",
    "2. def __init__(우리가 쓰고 싶은 Linear의 이름을 쭉 써주면 된다.), def forward 반드시 넣어줘야 한다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass 모델\n",
    "\n",
    "class LinearRegressionModel(nn.Module): # torch.nn.Module을 상속받는 파이썬 클래스\n",
    "    def __init__(self): #\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(3, 3)\n",
    "        self.linear2 = nn.Linear(3, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.5548,  0.3410, -0.0780],\n",
       "         [-0.2435,  0.2367, -0.0652],\n",
       "         [ 0.1537,  0.4814,  0.1286]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.4232,  0.1205, -0.1332], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.4916, -0.2149,  0.4012]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2917], requires_grad=True)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters()) # bias까지 총 10개의 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer  = optim.Adam(model.parameters(), lr = 4e-3) # 손실이 잘 줄어들지 않는다면 lr을 키워준다. 근데 또 너무 크면 막판에 잘 안 줄어들 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 32498.234375\n",
      "Epoch    1/1000 Cost: 31971.839844\n",
      "Epoch    2/1000 Cost: 31449.869141\n",
      "Epoch    3/1000 Cost: 30932.394531\n",
      "Epoch    4/1000 Cost: 30419.474609\n",
      "Epoch    5/1000 Cost: 29911.175781\n",
      "Epoch    6/1000 Cost: 29407.558594\n",
      "Epoch    7/1000 Cost: 28908.671875\n",
      "Epoch    8/1000 Cost: 28414.558594\n",
      "Epoch    9/1000 Cost: 27925.265625\n",
      "Epoch   10/1000 Cost: 27440.818359\n",
      "Epoch   11/1000 Cost: 26961.250000\n",
      "Epoch   12/1000 Cost: 26486.587891\n",
      "Epoch   13/1000 Cost: 26016.839844\n",
      "Epoch   14/1000 Cost: 25552.035156\n",
      "Epoch   15/1000 Cost: 25092.167969\n",
      "Epoch   16/1000 Cost: 24637.246094\n",
      "Epoch   17/1000 Cost: 24187.271484\n",
      "Epoch   18/1000 Cost: 23742.240234\n",
      "Epoch   19/1000 Cost: 23302.140625\n",
      "Epoch   20/1000 Cost: 22866.962891\n",
      "Epoch   21/1000 Cost: 22436.691406\n",
      "Epoch   22/1000 Cost: 22011.300781\n",
      "Epoch   23/1000 Cost: 21590.779297\n",
      "Epoch   24/1000 Cost: 21175.097656\n",
      "Epoch   25/1000 Cost: 20764.226562\n",
      "Epoch   26/1000 Cost: 20358.146484\n",
      "Epoch   27/1000 Cost: 19956.822266\n",
      "Epoch   28/1000 Cost: 19560.218750\n",
      "Epoch   29/1000 Cost: 19168.308594\n",
      "Epoch   30/1000 Cost: 18781.056641\n",
      "Epoch   31/1000 Cost: 18398.429688\n",
      "Epoch   32/1000 Cost: 18020.390625\n",
      "Epoch   33/1000 Cost: 17646.908203\n",
      "Epoch   34/1000 Cost: 17277.943359\n",
      "Epoch   35/1000 Cost: 16913.468750\n",
      "Epoch   36/1000 Cost: 16553.443359\n",
      "Epoch   37/1000 Cost: 16197.838867\n",
      "Epoch   38/1000 Cost: 15846.619141\n",
      "Epoch   39/1000 Cost: 15499.752930\n",
      "Epoch   40/1000 Cost: 15157.212891\n",
      "Epoch   41/1000 Cost: 14818.965820\n",
      "Epoch   42/1000 Cost: 14484.987305\n",
      "Epoch   43/1000 Cost: 14155.247070\n",
      "Epoch   44/1000 Cost: 13829.720703\n",
      "Epoch   45/1000 Cost: 13508.384766\n",
      "Epoch   46/1000 Cost: 13191.216797\n",
      "Epoch   47/1000 Cost: 12878.197266\n",
      "Epoch   48/1000 Cost: 12569.303711\n",
      "Epoch   49/1000 Cost: 12264.522461\n",
      "Epoch   50/1000 Cost: 11963.837891\n",
      "Epoch   51/1000 Cost: 11667.229492\n",
      "Epoch   52/1000 Cost: 11374.690430\n",
      "Epoch   53/1000 Cost: 11086.208984\n",
      "Epoch   54/1000 Cost: 10801.771484\n",
      "Epoch   55/1000 Cost: 10521.372070\n",
      "Epoch   56/1000 Cost: 10245.005859\n",
      "Epoch   57/1000 Cost: 9972.662109\n",
      "Epoch   58/1000 Cost: 9704.341797\n",
      "Epoch   59/1000 Cost: 9440.040039\n",
      "Epoch   60/1000 Cost: 9179.751953\n",
      "Epoch   61/1000 Cost: 8923.477539\n",
      "Epoch   62/1000 Cost: 8671.218750\n",
      "Epoch   63/1000 Cost: 8422.974609\n",
      "Epoch   64/1000 Cost: 8178.745117\n",
      "Epoch   65/1000 Cost: 7938.536133\n",
      "Epoch   66/1000 Cost: 7702.345215\n",
      "Epoch   67/1000 Cost: 7470.174805\n",
      "Epoch   68/1000 Cost: 7242.031250\n",
      "Epoch   69/1000 Cost: 7017.915527\n",
      "Epoch   70/1000 Cost: 6797.829102\n",
      "Epoch   71/1000 Cost: 6581.776367\n",
      "Epoch   72/1000 Cost: 6369.758301\n",
      "Epoch   73/1000 Cost: 6161.777832\n",
      "Epoch   74/1000 Cost: 5957.835449\n",
      "Epoch   75/1000 Cost: 5757.932617\n",
      "Epoch   76/1000 Cost: 5562.067871\n",
      "Epoch   77/1000 Cost: 5370.242188\n",
      "Epoch   78/1000 Cost: 5182.451172\n",
      "Epoch   79/1000 Cost: 4998.694336\n",
      "Epoch   80/1000 Cost: 4818.965820\n",
      "Epoch   81/1000 Cost: 4643.258789\n",
      "Epoch   82/1000 Cost: 4471.567871\n",
      "Epoch   83/1000 Cost: 4303.883789\n",
      "Epoch   84/1000 Cost: 4140.194336\n",
      "Epoch   85/1000 Cost: 3980.490723\n",
      "Epoch   86/1000 Cost: 3824.755859\n",
      "Epoch   87/1000 Cost: 3672.975830\n",
      "Epoch   88/1000 Cost: 3525.131348\n",
      "Epoch   89/1000 Cost: 3381.201904\n",
      "Epoch   90/1000 Cost: 3241.167236\n",
      "Epoch   91/1000 Cost: 3105.000732\n",
      "Epoch   92/1000 Cost: 2974.679443\n",
      "Epoch   93/1000 Cost: 2849.463623\n",
      "Epoch   94/1000 Cost: 2731.205078\n",
      "Epoch   95/1000 Cost: 2619.864746\n",
      "Epoch   96/1000 Cost: 2511.691650\n",
      "Epoch   97/1000 Cost: 2407.280518\n",
      "Epoch   98/1000 Cost: 2306.339355\n",
      "Epoch   99/1000 Cost: 2208.304932\n",
      "Epoch  100/1000 Cost: 2113.147949\n",
      "Epoch  101/1000 Cost: 2020.839478\n",
      "Epoch  102/1000 Cost: 1931.346436\n",
      "Epoch  103/1000 Cost: 1844.637329\n",
      "Epoch  104/1000 Cost: 1760.677979\n",
      "Epoch  105/1000 Cost: 1679.433960\n",
      "Epoch  106/1000 Cost: 1600.870972\n",
      "Epoch  107/1000 Cost: 1524.950928\n",
      "Epoch  108/1000 Cost: 1451.636353\n",
      "Epoch  109/1000 Cost: 1380.886108\n",
      "Epoch  110/1000 Cost: 1312.660889\n",
      "Epoch  111/1000 Cost: 1246.918579\n",
      "Epoch  112/1000 Cost: 1183.615967\n",
      "Epoch  113/1000 Cost: 1122.706299\n",
      "Epoch  114/1000 Cost: 1064.148193\n",
      "Epoch  115/1000 Cost: 1007.890991\n",
      "Epoch  116/1000 Cost: 953.887878\n",
      "Epoch  117/1000 Cost: 902.091431\n",
      "Epoch  118/1000 Cost: 852.450195\n",
      "Epoch  119/1000 Cost: 804.913818\n",
      "Epoch  120/1000 Cost: 759.432068\n",
      "Epoch  121/1000 Cost: 715.951416\n",
      "Epoch  122/1000 Cost: 674.418823\n",
      "Epoch  123/1000 Cost: 634.783997\n",
      "Epoch  124/1000 Cost: 596.990540\n",
      "Epoch  125/1000 Cost: 560.985901\n",
      "Epoch  126/1000 Cost: 526.715942\n",
      "Epoch  127/1000 Cost: 494.126221\n",
      "Epoch  128/1000 Cost: 463.162750\n",
      "Epoch  129/1000 Cost: 433.770905\n",
      "Epoch  130/1000 Cost: 405.898163\n",
      "Epoch  131/1000 Cost: 379.489807\n",
      "Epoch  132/1000 Cost: 354.492859\n",
      "Epoch  133/1000 Cost: 330.855560\n",
      "Epoch  134/1000 Cost: 308.523987\n",
      "Epoch  135/1000 Cost: 287.447845\n",
      "Epoch  136/1000 Cost: 267.575867\n",
      "Epoch  137/1000 Cost: 248.858002\n",
      "Epoch  138/1000 Cost: 231.244873\n",
      "Epoch  139/1000 Cost: 214.688141\n",
      "Epoch  140/1000 Cost: 199.140839\n",
      "Epoch  141/1000 Cost: 184.555862\n",
      "Epoch  142/1000 Cost: 170.888824\n",
      "Epoch  143/1000 Cost: 158.093994\n",
      "Epoch  144/1000 Cost: 146.130447\n",
      "Epoch  145/1000 Cost: 134.954987\n",
      "Epoch  146/1000 Cost: 124.527039\n",
      "Epoch  147/1000 Cost: 114.808044\n",
      "Epoch  148/1000 Cost: 105.759132\n",
      "Epoch  149/1000 Cost: 97.343430\n",
      "Epoch  150/1000 Cost: 89.526230\n",
      "Epoch  151/1000 Cost: 82.272850\n",
      "Epoch  152/1000 Cost: 75.550919\n",
      "Epoch  153/1000 Cost: 69.327728\n",
      "Epoch  154/1000 Cost: 63.574097\n",
      "Epoch  155/1000 Cost: 58.260639\n",
      "Epoch  156/1000 Cost: 53.359821\n",
      "Epoch  157/1000 Cost: 48.844852\n",
      "Epoch  158/1000 Cost: 44.690269\n",
      "Epoch  159/1000 Cost: 40.873009\n",
      "Epoch  160/1000 Cost: 37.369495\n",
      "Epoch  161/1000 Cost: 34.158180\n",
      "Epoch  162/1000 Cost: 31.218582\n",
      "Epoch  163/1000 Cost: 28.531427\n",
      "Epoch  164/1000 Cost: 26.078411\n",
      "Epoch  165/1000 Cost: 23.841990\n",
      "Epoch  166/1000 Cost: 21.806068\n",
      "Epoch  167/1000 Cost: 19.955265\n",
      "Epoch  168/1000 Cost: 18.275091\n",
      "Epoch  169/1000 Cost: 16.752268\n",
      "Epoch  170/1000 Cost: 15.374023\n",
      "Epoch  171/1000 Cost: 14.128512\n",
      "Epoch  172/1000 Cost: 13.004850\n",
      "Epoch  173/1000 Cost: 11.992677\n",
      "Epoch  174/1000 Cost: 11.082477\n",
      "Epoch  175/1000 Cost: 10.265339\n",
      "Epoch  176/1000 Cost: 9.533038\n",
      "Epoch  177/1000 Cost: 8.877966\n",
      "Epoch  178/1000 Cost: 8.292936\n",
      "Epoch  179/1000 Cost: 7.771523\n",
      "Epoch  180/1000 Cost: 7.307788\n",
      "Epoch  181/1000 Cost: 6.896089\n",
      "Epoch  182/1000 Cost: 6.531337\n",
      "Epoch  183/1000 Cost: 6.208844\n",
      "Epoch  184/1000 Cost: 5.924555\n",
      "Epoch  185/1000 Cost: 5.674343\n",
      "Epoch  186/1000 Cost: 5.454700\n",
      "Epoch  187/1000 Cost: 5.262438\n",
      "Epoch  188/1000 Cost: 5.094493\n",
      "Epoch  189/1000 Cost: 4.948362\n",
      "Epoch  190/1000 Cost: 4.821406\n",
      "Epoch  191/1000 Cost: 4.711632\n",
      "Epoch  192/1000 Cost: 4.616868\n",
      "Epoch  193/1000 Cost: 4.535420\n",
      "Epoch  194/1000 Cost: 4.465721\n",
      "Epoch  195/1000 Cost: 4.406278\n",
      "Epoch  196/1000 Cost: 4.355760\n",
      "Epoch  197/1000 Cost: 4.313123\n",
      "Epoch  198/1000 Cost: 4.277285\n",
      "Epoch  199/1000 Cost: 4.247294\n",
      "Epoch  200/1000 Cost: 4.222425\n",
      "Epoch  201/1000 Cost: 4.201873\n",
      "Epoch  202/1000 Cost: 4.185081\n",
      "Epoch  203/1000 Cost: 4.171476\n",
      "Epoch  204/1000 Cost: 4.160567\n",
      "Epoch  205/1000 Cost: 4.151870\n",
      "Epoch  206/1000 Cost: 4.145122\n",
      "Epoch  207/1000 Cost: 4.139981\n",
      "Epoch  208/1000 Cost: 4.136104\n",
      "Epoch  209/1000 Cost: 4.133269\n",
      "Epoch  210/1000 Cost: 4.131341\n",
      "Epoch  211/1000 Cost: 4.130087\n",
      "Epoch  212/1000 Cost: 4.129355\n",
      "Epoch  213/1000 Cost: 4.129045\n",
      "Epoch  214/1000 Cost: 4.129037\n",
      "Epoch  215/1000 Cost: 4.129288\n",
      "Epoch  216/1000 Cost: 4.129659\n",
      "Epoch  217/1000 Cost: 4.130148\n",
      "Epoch  218/1000 Cost: 4.130680\n",
      "Epoch  219/1000 Cost: 4.131217\n",
      "Epoch  220/1000 Cost: 4.131760\n",
      "Epoch  221/1000 Cost: 4.132249\n",
      "Epoch  222/1000 Cost: 4.132728\n",
      "Epoch  223/1000 Cost: 4.133060\n",
      "Epoch  224/1000 Cost: 4.133405\n",
      "Epoch  225/1000 Cost: 4.133676\n",
      "Epoch  226/1000 Cost: 4.133856\n",
      "Epoch  227/1000 Cost: 4.133972\n",
      "Epoch  228/1000 Cost: 4.133980\n",
      "Epoch  229/1000 Cost: 4.133960\n",
      "Epoch  230/1000 Cost: 4.133893\n",
      "Epoch  231/1000 Cost: 4.133770\n",
      "Epoch  232/1000 Cost: 4.133590\n",
      "Epoch  233/1000 Cost: 4.133363\n",
      "Epoch  234/1000 Cost: 4.133075\n",
      "Epoch  235/1000 Cost: 4.132817\n",
      "Epoch  236/1000 Cost: 4.132490\n",
      "Epoch  237/1000 Cost: 4.132136\n",
      "Epoch  238/1000 Cost: 4.131801\n",
      "Epoch  239/1000 Cost: 4.131436\n",
      "Epoch  240/1000 Cost: 4.131049\n",
      "Epoch  241/1000 Cost: 4.130678\n",
      "Epoch  242/1000 Cost: 4.130277\n",
      "Epoch  243/1000 Cost: 4.129893\n",
      "Epoch  244/1000 Cost: 4.129546\n",
      "Epoch  245/1000 Cost: 4.129146\n",
      "Epoch  246/1000 Cost: 4.128817\n",
      "Epoch  247/1000 Cost: 4.128448\n",
      "Epoch  248/1000 Cost: 4.128117\n",
      "Epoch  249/1000 Cost: 4.127770\n",
      "Epoch  250/1000 Cost: 4.127476\n",
      "Epoch  251/1000 Cost: 4.127145\n",
      "Epoch  252/1000 Cost: 4.126818\n",
      "Epoch  253/1000 Cost: 4.126557\n",
      "Epoch  254/1000 Cost: 4.126304\n",
      "Epoch  255/1000 Cost: 4.126017\n",
      "Epoch  256/1000 Cost: 4.125765\n",
      "Epoch  257/1000 Cost: 4.125525\n",
      "Epoch  258/1000 Cost: 4.125290\n",
      "Epoch  259/1000 Cost: 4.125079\n",
      "Epoch  260/1000 Cost: 4.124864\n",
      "Epoch  261/1000 Cost: 4.124660\n",
      "Epoch  262/1000 Cost: 4.124477\n",
      "Epoch  263/1000 Cost: 4.124292\n",
      "Epoch  264/1000 Cost: 4.124122\n",
      "Epoch  265/1000 Cost: 4.123930\n",
      "Epoch  266/1000 Cost: 4.123789\n",
      "Epoch  267/1000 Cost: 4.123624\n",
      "Epoch  268/1000 Cost: 4.123471\n",
      "Epoch  269/1000 Cost: 4.123319\n",
      "Epoch  270/1000 Cost: 4.123179\n",
      "Epoch  271/1000 Cost: 4.123037\n",
      "Epoch  272/1000 Cost: 4.122898\n",
      "Epoch  273/1000 Cost: 4.122776\n",
      "Epoch  274/1000 Cost: 4.122652\n",
      "Epoch  275/1000 Cost: 4.122523\n",
      "Epoch  276/1000 Cost: 4.122365\n",
      "Epoch  277/1000 Cost: 4.122280\n",
      "Epoch  278/1000 Cost: 4.122122\n",
      "Epoch  279/1000 Cost: 4.122012\n",
      "Epoch  280/1000 Cost: 4.121892\n",
      "Epoch  281/1000 Cost: 4.121764\n",
      "Epoch  282/1000 Cost: 4.121643\n",
      "Epoch  283/1000 Cost: 4.121533\n",
      "Epoch  284/1000 Cost: 4.121426\n",
      "Epoch  285/1000 Cost: 4.121295\n",
      "Epoch  286/1000 Cost: 4.121188\n",
      "Epoch  287/1000 Cost: 4.121068\n",
      "Epoch  288/1000 Cost: 4.120965\n",
      "Epoch  289/1000 Cost: 4.120808\n",
      "Epoch  290/1000 Cost: 4.120751\n",
      "Epoch  291/1000 Cost: 4.120578\n",
      "Epoch  292/1000 Cost: 4.120491\n",
      "Epoch  293/1000 Cost: 4.120373\n",
      "Epoch  294/1000 Cost: 4.120242\n",
      "Epoch  295/1000 Cost: 4.120116\n",
      "Epoch  296/1000 Cost: 4.120010\n",
      "Epoch  297/1000 Cost: 4.119895\n",
      "Epoch  298/1000 Cost: 4.119788\n",
      "Epoch  299/1000 Cost: 4.119630\n",
      "Epoch  300/1000 Cost: 4.119506\n",
      "Epoch  301/1000 Cost: 4.119421\n",
      "Epoch  302/1000 Cost: 4.119287\n",
      "Epoch  303/1000 Cost: 4.119195\n",
      "Epoch  304/1000 Cost: 4.119034\n",
      "Epoch  305/1000 Cost: 4.118943\n",
      "Epoch  306/1000 Cost: 4.118826\n",
      "Epoch  307/1000 Cost: 4.118685\n",
      "Epoch  308/1000 Cost: 4.118568\n",
      "Epoch  309/1000 Cost: 4.118440\n",
      "Epoch  310/1000 Cost: 4.118317\n",
      "Epoch  311/1000 Cost: 4.118197\n",
      "Epoch  312/1000 Cost: 4.118101\n",
      "Epoch  313/1000 Cost: 4.117965\n",
      "Epoch  314/1000 Cost: 4.117843\n",
      "Epoch  315/1000 Cost: 4.117705\n",
      "Epoch  316/1000 Cost: 4.117589\n",
      "Epoch  317/1000 Cost: 4.117449\n",
      "Epoch  318/1000 Cost: 4.117346\n",
      "Epoch  319/1000 Cost: 4.117204\n",
      "Epoch  320/1000 Cost: 4.117096\n",
      "Epoch  321/1000 Cost: 4.116987\n",
      "Epoch  322/1000 Cost: 4.116836\n",
      "Epoch  323/1000 Cost: 4.116721\n",
      "Epoch  324/1000 Cost: 4.116584\n",
      "Epoch  325/1000 Cost: 4.116487\n",
      "Epoch  326/1000 Cost: 4.116346\n",
      "Epoch  327/1000 Cost: 4.116222\n",
      "Epoch  328/1000 Cost: 4.116106\n",
      "Epoch  329/1000 Cost: 4.115968\n",
      "Epoch  330/1000 Cost: 4.115837\n",
      "Epoch  331/1000 Cost: 4.115731\n",
      "Epoch  332/1000 Cost: 4.115587\n",
      "Epoch  333/1000 Cost: 4.115469\n",
      "Epoch  334/1000 Cost: 4.115328\n",
      "Epoch  335/1000 Cost: 4.115200\n",
      "Epoch  336/1000 Cost: 4.115071\n",
      "Epoch  337/1000 Cost: 4.114962\n",
      "Epoch  338/1000 Cost: 4.114834\n",
      "Epoch  339/1000 Cost: 4.114711\n",
      "Epoch  340/1000 Cost: 4.114575\n",
      "Epoch  341/1000 Cost: 4.114444\n",
      "Epoch  342/1000 Cost: 4.114319\n",
      "Epoch  343/1000 Cost: 4.114188\n",
      "Epoch  344/1000 Cost: 4.114053\n",
      "Epoch  345/1000 Cost: 4.113915\n",
      "Epoch  346/1000 Cost: 4.113791\n",
      "Epoch  347/1000 Cost: 4.113664\n",
      "Epoch  348/1000 Cost: 4.113537\n",
      "Epoch  349/1000 Cost: 4.113406\n",
      "Epoch  350/1000 Cost: 4.113261\n",
      "Epoch  351/1000 Cost: 4.113135\n",
      "Epoch  352/1000 Cost: 4.113009\n",
      "Epoch  353/1000 Cost: 4.112864\n",
      "Epoch  354/1000 Cost: 4.112720\n",
      "Epoch  355/1000 Cost: 4.112615\n",
      "Epoch  356/1000 Cost: 4.112472\n",
      "Epoch  357/1000 Cost: 4.112311\n",
      "Epoch  358/1000 Cost: 4.112236\n",
      "Epoch  359/1000 Cost: 4.112084\n",
      "Epoch  360/1000 Cost: 4.111922\n",
      "Epoch  361/1000 Cost: 4.111834\n",
      "Epoch  362/1000 Cost: 4.111692\n",
      "Epoch  363/1000 Cost: 4.111557\n",
      "Epoch  364/1000 Cost: 4.111424\n",
      "Epoch  365/1000 Cost: 4.111284\n",
      "Epoch  366/1000 Cost: 4.111156\n",
      "Epoch  367/1000 Cost: 4.111010\n",
      "Epoch  368/1000 Cost: 4.110921\n",
      "Epoch  369/1000 Cost: 4.110753\n",
      "Epoch  370/1000 Cost: 4.110631\n",
      "Epoch  371/1000 Cost: 4.110486\n",
      "Epoch  372/1000 Cost: 4.110361\n",
      "Epoch  373/1000 Cost: 4.110196\n",
      "Epoch  374/1000 Cost: 4.110073\n",
      "Epoch  375/1000 Cost: 4.109930\n",
      "Epoch  376/1000 Cost: 4.109803\n",
      "Epoch  377/1000 Cost: 4.109642\n",
      "Epoch  378/1000 Cost: 4.109538\n",
      "Epoch  379/1000 Cost: 4.109404\n",
      "Epoch  380/1000 Cost: 4.109263\n",
      "Epoch  381/1000 Cost: 4.109127\n",
      "Epoch  382/1000 Cost: 4.108996\n",
      "Epoch  383/1000 Cost: 4.108799\n",
      "Epoch  384/1000 Cost: 4.108687\n",
      "Epoch  385/1000 Cost: 4.108562\n",
      "Epoch  386/1000 Cost: 4.108423\n",
      "Epoch  387/1000 Cost: 4.108314\n",
      "Epoch  388/1000 Cost: 4.108137\n",
      "Epoch  389/1000 Cost: 4.107989\n",
      "Epoch  390/1000 Cost: 4.107867\n",
      "Epoch  391/1000 Cost: 4.107714\n",
      "Epoch  392/1000 Cost: 4.107594\n",
      "Epoch  393/1000 Cost: 4.107449\n",
      "Epoch  394/1000 Cost: 4.107310\n",
      "Epoch  395/1000 Cost: 4.107169\n",
      "Epoch  396/1000 Cost: 4.107049\n",
      "Epoch  397/1000 Cost: 4.106885\n",
      "Epoch  398/1000 Cost: 4.106723\n",
      "Epoch  399/1000 Cost: 4.106591\n",
      "Epoch  400/1000 Cost: 4.106475\n",
      "Epoch  401/1000 Cost: 4.106359\n",
      "Epoch  402/1000 Cost: 4.106176\n",
      "Epoch  403/1000 Cost: 4.106034\n",
      "Epoch  404/1000 Cost: 4.105917\n",
      "Epoch  405/1000 Cost: 4.105762\n",
      "Epoch  406/1000 Cost: 4.105620\n",
      "Epoch  407/1000 Cost: 4.105477\n",
      "Epoch  408/1000 Cost: 4.105330\n",
      "Epoch  409/1000 Cost: 4.105206\n",
      "Epoch  410/1000 Cost: 4.105056\n",
      "Epoch  411/1000 Cost: 4.104904\n",
      "Epoch  412/1000 Cost: 4.104761\n",
      "Epoch  413/1000 Cost: 4.104620\n",
      "Epoch  414/1000 Cost: 4.104455\n",
      "Epoch  415/1000 Cost: 4.104319\n",
      "Epoch  416/1000 Cost: 4.104198\n",
      "Epoch  417/1000 Cost: 4.104049\n",
      "Epoch  418/1000 Cost: 4.103900\n",
      "Epoch  419/1000 Cost: 4.103728\n",
      "Epoch  420/1000 Cost: 4.103611\n",
      "Epoch  421/1000 Cost: 4.103470\n",
      "Epoch  422/1000 Cost: 4.103303\n",
      "Epoch  423/1000 Cost: 4.103165\n",
      "Epoch  424/1000 Cost: 4.103017\n",
      "Epoch  425/1000 Cost: 4.102852\n",
      "Epoch  426/1000 Cost: 4.102735\n",
      "Epoch  427/1000 Cost: 4.102578\n",
      "Epoch  428/1000 Cost: 4.102407\n",
      "Epoch  429/1000 Cost: 4.102295\n",
      "Epoch  430/1000 Cost: 4.102146\n",
      "Epoch  431/1000 Cost: 4.101991\n",
      "Epoch  432/1000 Cost: 4.101821\n",
      "Epoch  433/1000 Cost: 4.101680\n",
      "Epoch  434/1000 Cost: 4.101545\n",
      "Epoch  435/1000 Cost: 4.101414\n",
      "Epoch  436/1000 Cost: 4.101281\n",
      "Epoch  437/1000 Cost: 4.101087\n",
      "Epoch  438/1000 Cost: 4.100964\n",
      "Epoch  439/1000 Cost: 4.100795\n",
      "Epoch  440/1000 Cost: 4.100639\n",
      "Epoch  441/1000 Cost: 4.100520\n",
      "Epoch  442/1000 Cost: 4.100350\n",
      "Epoch  443/1000 Cost: 4.100209\n",
      "Epoch  444/1000 Cost: 4.100034\n",
      "Epoch  445/1000 Cost: 4.099904\n",
      "Epoch  446/1000 Cost: 4.099751\n",
      "Epoch  447/1000 Cost: 4.099595\n",
      "Epoch  448/1000 Cost: 4.099451\n",
      "Epoch  449/1000 Cost: 4.099301\n",
      "Epoch  450/1000 Cost: 4.099140\n",
      "Epoch  451/1000 Cost: 4.098977\n",
      "Epoch  452/1000 Cost: 4.098845\n",
      "Epoch  453/1000 Cost: 4.098719\n",
      "Epoch  454/1000 Cost: 4.098535\n",
      "Epoch  455/1000 Cost: 4.098372\n",
      "Epoch  456/1000 Cost: 4.098217\n",
      "Epoch  457/1000 Cost: 4.098075\n",
      "Epoch  458/1000 Cost: 4.097928\n",
      "Epoch  459/1000 Cost: 4.097773\n",
      "Epoch  460/1000 Cost: 4.097638\n",
      "Epoch  461/1000 Cost: 4.097478\n",
      "Epoch  462/1000 Cost: 4.097304\n",
      "Epoch  463/1000 Cost: 4.097162\n",
      "Epoch  464/1000 Cost: 4.096994\n",
      "Epoch  465/1000 Cost: 4.096859\n",
      "Epoch  466/1000 Cost: 4.096677\n",
      "Epoch  467/1000 Cost: 4.096537\n",
      "Epoch  468/1000 Cost: 4.096395\n",
      "Epoch  469/1000 Cost: 4.096241\n",
      "Epoch  470/1000 Cost: 4.096063\n",
      "Epoch  471/1000 Cost: 4.095936\n",
      "Epoch  472/1000 Cost: 4.095750\n",
      "Epoch  473/1000 Cost: 4.095598\n",
      "Epoch  474/1000 Cost: 4.095432\n",
      "Epoch  475/1000 Cost: 4.095302\n",
      "Epoch  476/1000 Cost: 4.095160\n",
      "Epoch  477/1000 Cost: 4.094973\n",
      "Epoch  478/1000 Cost: 4.094825\n",
      "Epoch  479/1000 Cost: 4.094633\n",
      "Epoch  480/1000 Cost: 4.094512\n",
      "Epoch  481/1000 Cost: 4.094344\n",
      "Epoch  482/1000 Cost: 4.094201\n",
      "Epoch  483/1000 Cost: 4.094039\n",
      "Epoch  484/1000 Cost: 4.093883\n",
      "Epoch  485/1000 Cost: 4.093723\n",
      "Epoch  486/1000 Cost: 4.093558\n",
      "Epoch  487/1000 Cost: 4.093410\n",
      "Epoch  488/1000 Cost: 4.093229\n",
      "Epoch  489/1000 Cost: 4.093072\n",
      "Epoch  490/1000 Cost: 4.092930\n",
      "Epoch  491/1000 Cost: 4.092755\n",
      "Epoch  492/1000 Cost: 4.092605\n",
      "Epoch  493/1000 Cost: 4.092434\n",
      "Epoch  494/1000 Cost: 4.092281\n",
      "Epoch  495/1000 Cost: 4.092132\n",
      "Epoch  496/1000 Cost: 4.091960\n",
      "Epoch  497/1000 Cost: 4.091814\n",
      "Epoch  498/1000 Cost: 4.091650\n",
      "Epoch  499/1000 Cost: 4.091483\n",
      "Epoch  500/1000 Cost: 4.091308\n",
      "Epoch  501/1000 Cost: 4.091172\n",
      "Epoch  502/1000 Cost: 4.090999\n",
      "Epoch  503/1000 Cost: 4.090834\n",
      "Epoch  504/1000 Cost: 4.090681\n",
      "Epoch  505/1000 Cost: 4.090524\n",
      "Epoch  506/1000 Cost: 4.090369\n",
      "Epoch  507/1000 Cost: 4.090171\n",
      "Epoch  508/1000 Cost: 4.090007\n",
      "Epoch  509/1000 Cost: 4.089845\n",
      "Epoch  510/1000 Cost: 4.089692\n",
      "Epoch  511/1000 Cost: 4.089508\n",
      "Epoch  512/1000 Cost: 4.089351\n",
      "Epoch  513/1000 Cost: 4.089195\n",
      "Epoch  514/1000 Cost: 4.089029\n",
      "Epoch  515/1000 Cost: 4.088874\n",
      "Epoch  516/1000 Cost: 4.088694\n",
      "Epoch  517/1000 Cost: 4.088553\n",
      "Epoch  518/1000 Cost: 4.088376\n",
      "Epoch  519/1000 Cost: 4.088193\n",
      "Epoch  520/1000 Cost: 4.088031\n",
      "Epoch  521/1000 Cost: 4.087891\n",
      "Epoch  522/1000 Cost: 4.087705\n",
      "Epoch  523/1000 Cost: 4.087539\n",
      "Epoch  524/1000 Cost: 4.087372\n",
      "Epoch  525/1000 Cost: 4.087218\n",
      "Epoch  526/1000 Cost: 4.087047\n",
      "Epoch  527/1000 Cost: 4.086878\n",
      "Epoch  528/1000 Cost: 4.086715\n",
      "Epoch  529/1000 Cost: 4.086543\n",
      "Epoch  530/1000 Cost: 4.086395\n",
      "Epoch  531/1000 Cost: 4.086204\n",
      "Epoch  532/1000 Cost: 4.086049\n",
      "Epoch  533/1000 Cost: 4.085883\n",
      "Epoch  534/1000 Cost: 4.085723\n",
      "Epoch  535/1000 Cost: 4.085543\n",
      "Epoch  536/1000 Cost: 4.085354\n",
      "Epoch  537/1000 Cost: 4.085207\n",
      "Epoch  538/1000 Cost: 4.085034\n",
      "Epoch  539/1000 Cost: 4.084873\n",
      "Epoch  540/1000 Cost: 4.084688\n",
      "Epoch  541/1000 Cost: 4.084522\n",
      "Epoch  542/1000 Cost: 4.084350\n",
      "Epoch  543/1000 Cost: 4.084220\n",
      "Epoch  544/1000 Cost: 4.084022\n",
      "Epoch  545/1000 Cost: 4.083830\n",
      "Epoch  546/1000 Cost: 4.083657\n",
      "Epoch  547/1000 Cost: 4.083478\n",
      "Epoch  548/1000 Cost: 4.083331\n",
      "Epoch  549/1000 Cost: 4.083168\n",
      "Epoch  550/1000 Cost: 4.082967\n",
      "Epoch  551/1000 Cost: 4.082820\n",
      "Epoch  552/1000 Cost: 4.082627\n",
      "Epoch  553/1000 Cost: 4.082492\n",
      "Epoch  554/1000 Cost: 4.082307\n",
      "Epoch  555/1000 Cost: 4.082116\n",
      "Epoch  556/1000 Cost: 4.081944\n",
      "Epoch  557/1000 Cost: 4.081796\n",
      "Epoch  558/1000 Cost: 4.081605\n",
      "Epoch  559/1000 Cost: 4.081469\n",
      "Epoch  560/1000 Cost: 4.081241\n",
      "Epoch  561/1000 Cost: 4.081095\n",
      "Epoch  562/1000 Cost: 4.080916\n",
      "Epoch  563/1000 Cost: 4.080743\n",
      "Epoch  564/1000 Cost: 4.080554\n",
      "Epoch  565/1000 Cost: 4.080418\n",
      "Epoch  566/1000 Cost: 4.080235\n",
      "Epoch  567/1000 Cost: 4.080033\n",
      "Epoch  568/1000 Cost: 4.079878\n",
      "Epoch  569/1000 Cost: 4.079703\n",
      "Epoch  570/1000 Cost: 4.079532\n",
      "Epoch  571/1000 Cost: 4.079341\n",
      "Epoch  572/1000 Cost: 4.079163\n",
      "Epoch  573/1000 Cost: 4.078984\n",
      "Epoch  574/1000 Cost: 4.078839\n",
      "Epoch  575/1000 Cost: 4.078630\n",
      "Epoch  576/1000 Cost: 4.078445\n",
      "Epoch  577/1000 Cost: 4.078286\n",
      "Epoch  578/1000 Cost: 4.078099\n",
      "Epoch  579/1000 Cost: 4.077947\n",
      "Epoch  580/1000 Cost: 4.077754\n",
      "Epoch  581/1000 Cost: 4.077613\n",
      "Epoch  582/1000 Cost: 4.077386\n",
      "Epoch  583/1000 Cost: 4.077239\n",
      "Epoch  584/1000 Cost: 4.077048\n",
      "Epoch  585/1000 Cost: 4.076875\n",
      "Epoch  586/1000 Cost: 4.076690\n",
      "Epoch  587/1000 Cost: 4.076537\n",
      "Epoch  588/1000 Cost: 4.076344\n",
      "Epoch  589/1000 Cost: 4.076180\n",
      "Epoch  590/1000 Cost: 4.075962\n",
      "Epoch  591/1000 Cost: 4.075778\n",
      "Epoch  592/1000 Cost: 4.075618\n",
      "Epoch  593/1000 Cost: 4.075465\n",
      "Epoch  594/1000 Cost: 4.075258\n",
      "Epoch  595/1000 Cost: 4.075088\n",
      "Epoch  596/1000 Cost: 4.074899\n",
      "Epoch  597/1000 Cost: 4.074731\n",
      "Epoch  598/1000 Cost: 4.074530\n",
      "Epoch  599/1000 Cost: 4.074366\n",
      "Epoch  600/1000 Cost: 4.074173\n",
      "Epoch  601/1000 Cost: 4.073985\n",
      "Epoch  602/1000 Cost: 4.073809\n",
      "Epoch  603/1000 Cost: 4.073643\n",
      "Epoch  604/1000 Cost: 4.073447\n",
      "Epoch  605/1000 Cost: 4.073260\n",
      "Epoch  606/1000 Cost: 4.073108\n",
      "Epoch  607/1000 Cost: 4.072900\n",
      "Epoch  608/1000 Cost: 4.072715\n",
      "Epoch  609/1000 Cost: 4.072541\n",
      "Epoch  610/1000 Cost: 4.072359\n",
      "Epoch  611/1000 Cost: 4.072160\n",
      "Epoch  612/1000 Cost: 4.072008\n",
      "Epoch  613/1000 Cost: 4.071807\n",
      "Epoch  614/1000 Cost: 4.071633\n",
      "Epoch  615/1000 Cost: 4.071439\n",
      "Epoch  616/1000 Cost: 4.071275\n",
      "Epoch  617/1000 Cost: 4.071045\n",
      "Epoch  618/1000 Cost: 4.070882\n",
      "Epoch  619/1000 Cost: 4.070720\n",
      "Epoch  620/1000 Cost: 4.070527\n",
      "Epoch  621/1000 Cost: 4.070332\n",
      "Epoch  622/1000 Cost: 4.070135\n",
      "Epoch  623/1000 Cost: 4.069961\n",
      "Epoch  624/1000 Cost: 4.069784\n",
      "Epoch  625/1000 Cost: 4.069607\n",
      "Epoch  626/1000 Cost: 4.069424\n",
      "Epoch  627/1000 Cost: 4.069205\n",
      "Epoch  628/1000 Cost: 4.069034\n",
      "Epoch  629/1000 Cost: 4.068855\n",
      "Epoch  630/1000 Cost: 4.068652\n",
      "Epoch  631/1000 Cost: 4.068489\n",
      "Epoch  632/1000 Cost: 4.068307\n",
      "Epoch  633/1000 Cost: 4.068093\n",
      "Epoch  634/1000 Cost: 4.067904\n",
      "Epoch  635/1000 Cost: 4.067712\n",
      "Epoch  636/1000 Cost: 4.067542\n",
      "Epoch  637/1000 Cost: 4.067344\n",
      "Epoch  638/1000 Cost: 4.067151\n",
      "Epoch  639/1000 Cost: 4.066960\n",
      "Epoch  640/1000 Cost: 4.066762\n",
      "Epoch  641/1000 Cost: 4.066578\n",
      "Epoch  642/1000 Cost: 4.066386\n",
      "Epoch  643/1000 Cost: 4.066216\n",
      "Epoch  644/1000 Cost: 4.066031\n",
      "Epoch  645/1000 Cost: 4.065862\n",
      "Epoch  646/1000 Cost: 4.065651\n",
      "Epoch  647/1000 Cost: 4.065428\n",
      "Epoch  648/1000 Cost: 4.065279\n",
      "Epoch  649/1000 Cost: 4.065078\n",
      "Epoch  650/1000 Cost: 4.064887\n",
      "Epoch  651/1000 Cost: 4.064691\n",
      "Epoch  652/1000 Cost: 4.064492\n",
      "Epoch  653/1000 Cost: 4.064305\n",
      "Epoch  654/1000 Cost: 4.064109\n",
      "Epoch  655/1000 Cost: 4.063941\n",
      "Epoch  656/1000 Cost: 4.063747\n",
      "Epoch  657/1000 Cost: 4.063565\n",
      "Epoch  658/1000 Cost: 4.063358\n",
      "Epoch  659/1000 Cost: 4.063159\n",
      "Epoch  660/1000 Cost: 4.062966\n",
      "Epoch  661/1000 Cost: 4.062765\n",
      "Epoch  662/1000 Cost: 4.062587\n",
      "Epoch  663/1000 Cost: 4.062394\n",
      "Epoch  664/1000 Cost: 4.062177\n",
      "Epoch  665/1000 Cost: 4.061991\n",
      "Epoch  666/1000 Cost: 4.061803\n",
      "Epoch  667/1000 Cost: 4.061636\n",
      "Epoch  668/1000 Cost: 4.061423\n",
      "Epoch  669/1000 Cost: 4.061228\n",
      "Epoch  670/1000 Cost: 4.061053\n",
      "Epoch  671/1000 Cost: 4.060825\n",
      "Epoch  672/1000 Cost: 4.060632\n",
      "Epoch  673/1000 Cost: 4.060466\n",
      "Epoch  674/1000 Cost: 4.060266\n",
      "Epoch  675/1000 Cost: 4.060081\n",
      "Epoch  676/1000 Cost: 4.059864\n",
      "Epoch  677/1000 Cost: 4.059701\n",
      "Epoch  678/1000 Cost: 4.059474\n",
      "Epoch  679/1000 Cost: 4.059274\n",
      "Epoch  680/1000 Cost: 4.059068\n",
      "Epoch  681/1000 Cost: 4.058867\n",
      "Epoch  682/1000 Cost: 4.058670\n",
      "Epoch  683/1000 Cost: 4.058479\n",
      "Epoch  684/1000 Cost: 4.058323\n",
      "Epoch  685/1000 Cost: 4.058093\n",
      "Epoch  686/1000 Cost: 4.057888\n",
      "Epoch  687/1000 Cost: 4.057695\n",
      "Epoch  688/1000 Cost: 4.057521\n",
      "Epoch  689/1000 Cost: 4.057284\n",
      "Epoch  690/1000 Cost: 4.057132\n",
      "Epoch  691/1000 Cost: 4.056923\n",
      "Epoch  692/1000 Cost: 4.056723\n",
      "Epoch  693/1000 Cost: 4.056534\n",
      "Epoch  694/1000 Cost: 4.056315\n",
      "Epoch  695/1000 Cost: 4.056102\n",
      "Epoch  696/1000 Cost: 4.055884\n",
      "Epoch  697/1000 Cost: 4.055729\n",
      "Epoch  698/1000 Cost: 4.055507\n",
      "Epoch  699/1000 Cost: 4.055340\n",
      "Epoch  700/1000 Cost: 4.055083\n",
      "Epoch  701/1000 Cost: 4.054899\n",
      "Epoch  702/1000 Cost: 4.054722\n",
      "Epoch  703/1000 Cost: 4.054526\n",
      "Epoch  704/1000 Cost: 4.054303\n",
      "Epoch  705/1000 Cost: 4.054111\n",
      "Epoch  706/1000 Cost: 4.053936\n",
      "Epoch  707/1000 Cost: 4.053709\n",
      "Epoch  708/1000 Cost: 4.053517\n",
      "Epoch  709/1000 Cost: 4.053302\n",
      "Epoch  710/1000 Cost: 4.053100\n",
      "Epoch  711/1000 Cost: 4.052913\n",
      "Epoch  712/1000 Cost: 4.052711\n",
      "Epoch  713/1000 Cost: 4.052494\n",
      "Epoch  714/1000 Cost: 4.052310\n",
      "Epoch  715/1000 Cost: 4.052094\n",
      "Epoch  716/1000 Cost: 4.051893\n",
      "Epoch  717/1000 Cost: 4.051679\n",
      "Epoch  718/1000 Cost: 4.051472\n",
      "Epoch  719/1000 Cost: 4.051279\n",
      "Epoch  720/1000 Cost: 4.051059\n",
      "Epoch  721/1000 Cost: 4.050877\n",
      "Epoch  722/1000 Cost: 4.050664\n",
      "Epoch  723/1000 Cost: 4.050484\n",
      "Epoch  724/1000 Cost: 4.050243\n",
      "Epoch  725/1000 Cost: 4.050056\n",
      "Epoch  726/1000 Cost: 4.049859\n",
      "Epoch  727/1000 Cost: 4.049666\n",
      "Epoch  728/1000 Cost: 4.049431\n",
      "Epoch  729/1000 Cost: 4.049242\n",
      "Epoch  730/1000 Cost: 4.049043\n",
      "Epoch  731/1000 Cost: 4.048826\n",
      "Epoch  732/1000 Cost: 4.048629\n",
      "Epoch  733/1000 Cost: 4.048430\n",
      "Epoch  734/1000 Cost: 4.048215\n",
      "Epoch  735/1000 Cost: 4.048003\n",
      "Epoch  736/1000 Cost: 4.047801\n",
      "Epoch  737/1000 Cost: 4.047590\n",
      "Epoch  738/1000 Cost: 4.047385\n",
      "Epoch  739/1000 Cost: 4.047190\n",
      "Epoch  740/1000 Cost: 4.046964\n",
      "Epoch  741/1000 Cost: 4.046766\n",
      "Epoch  742/1000 Cost: 4.046541\n",
      "Epoch  743/1000 Cost: 4.046343\n",
      "Epoch  744/1000 Cost: 4.046117\n",
      "Epoch  745/1000 Cost: 4.045924\n",
      "Epoch  746/1000 Cost: 4.045716\n",
      "Epoch  747/1000 Cost: 4.045509\n",
      "Epoch  748/1000 Cost: 4.045280\n",
      "Epoch  749/1000 Cost: 4.045129\n",
      "Epoch  750/1000 Cost: 4.044885\n",
      "Epoch  751/1000 Cost: 4.044680\n",
      "Epoch  752/1000 Cost: 4.044458\n",
      "Epoch  753/1000 Cost: 4.044246\n",
      "Epoch  754/1000 Cost: 4.044042\n",
      "Epoch  755/1000 Cost: 4.043831\n",
      "Epoch  756/1000 Cost: 4.043618\n",
      "Epoch  757/1000 Cost: 4.043412\n",
      "Epoch  758/1000 Cost: 4.043208\n",
      "Epoch  759/1000 Cost: 4.043004\n",
      "Epoch  760/1000 Cost: 4.042797\n",
      "Epoch  761/1000 Cost: 4.042575\n",
      "Epoch  762/1000 Cost: 4.042371\n",
      "Epoch  763/1000 Cost: 4.042144\n",
      "Epoch  764/1000 Cost: 4.041944\n",
      "Epoch  765/1000 Cost: 4.041732\n",
      "Epoch  766/1000 Cost: 4.041508\n",
      "Epoch  767/1000 Cost: 4.041286\n",
      "Epoch  768/1000 Cost: 4.041088\n",
      "Epoch  769/1000 Cost: 4.040881\n",
      "Epoch  770/1000 Cost: 4.040687\n",
      "Epoch  771/1000 Cost: 4.040442\n",
      "Epoch  772/1000 Cost: 4.040245\n",
      "Epoch  773/1000 Cost: 4.040008\n",
      "Epoch  774/1000 Cost: 4.039806\n",
      "Epoch  775/1000 Cost: 4.039584\n",
      "Epoch  776/1000 Cost: 4.039366\n",
      "Epoch  777/1000 Cost: 4.039179\n",
      "Epoch  778/1000 Cost: 4.038965\n",
      "Epoch  779/1000 Cost: 4.038722\n",
      "Epoch  780/1000 Cost: 4.038513\n",
      "Epoch  781/1000 Cost: 4.038321\n",
      "Epoch  782/1000 Cost: 4.038093\n",
      "Epoch  783/1000 Cost: 4.037885\n",
      "Epoch  784/1000 Cost: 4.037651\n",
      "Epoch  785/1000 Cost: 4.037442\n",
      "Epoch  786/1000 Cost: 4.037227\n",
      "Epoch  787/1000 Cost: 4.037027\n",
      "Epoch  788/1000 Cost: 4.036787\n",
      "Epoch  789/1000 Cost: 4.036571\n",
      "Epoch  790/1000 Cost: 4.036364\n",
      "Epoch  791/1000 Cost: 4.036164\n",
      "Epoch  792/1000 Cost: 4.035935\n",
      "Epoch  793/1000 Cost: 4.035690\n",
      "Epoch  794/1000 Cost: 4.035506\n",
      "Epoch  795/1000 Cost: 4.035276\n",
      "Epoch  796/1000 Cost: 4.035050\n",
      "Epoch  797/1000 Cost: 4.034848\n",
      "Epoch  798/1000 Cost: 4.034625\n",
      "Epoch  799/1000 Cost: 4.034430\n",
      "Epoch  800/1000 Cost: 4.034198\n",
      "Epoch  801/1000 Cost: 4.033977\n",
      "Epoch  802/1000 Cost: 4.033769\n",
      "Epoch  803/1000 Cost: 4.033519\n",
      "Epoch  804/1000 Cost: 4.033312\n",
      "Epoch  805/1000 Cost: 4.033110\n",
      "Epoch  806/1000 Cost: 4.032881\n",
      "Epoch  807/1000 Cost: 4.032637\n",
      "Epoch  808/1000 Cost: 4.032422\n",
      "Epoch  809/1000 Cost: 4.032222\n",
      "Epoch  810/1000 Cost: 4.031993\n",
      "Epoch  811/1000 Cost: 4.031781\n",
      "Epoch  812/1000 Cost: 4.031538\n",
      "Epoch  813/1000 Cost: 4.031305\n",
      "Epoch  814/1000 Cost: 4.031097\n",
      "Epoch  815/1000 Cost: 4.030901\n",
      "Epoch  816/1000 Cost: 4.030683\n",
      "Epoch  817/1000 Cost: 4.030429\n",
      "Epoch  818/1000 Cost: 4.030244\n",
      "Epoch  819/1000 Cost: 4.029986\n",
      "Epoch  820/1000 Cost: 4.029769\n",
      "Epoch  821/1000 Cost: 4.029552\n",
      "Epoch  822/1000 Cost: 4.029329\n",
      "Epoch  823/1000 Cost: 4.029096\n",
      "Epoch  824/1000 Cost: 4.028872\n",
      "Epoch  825/1000 Cost: 4.028630\n",
      "Epoch  826/1000 Cost: 4.028426\n",
      "Epoch  827/1000 Cost: 4.028220\n",
      "Epoch  828/1000 Cost: 4.027994\n",
      "Epoch  829/1000 Cost: 4.027761\n",
      "Epoch  830/1000 Cost: 4.027541\n",
      "Epoch  831/1000 Cost: 4.027349\n",
      "Epoch  832/1000 Cost: 4.027101\n",
      "Epoch  833/1000 Cost: 4.026891\n",
      "Epoch  834/1000 Cost: 4.026647\n",
      "Epoch  835/1000 Cost: 4.026414\n",
      "Epoch  836/1000 Cost: 4.026180\n",
      "Epoch  837/1000 Cost: 4.025980\n",
      "Epoch  838/1000 Cost: 4.025754\n",
      "Epoch  839/1000 Cost: 4.025530\n",
      "Epoch  840/1000 Cost: 4.025301\n",
      "Epoch  841/1000 Cost: 4.025070\n",
      "Epoch  842/1000 Cost: 4.024845\n",
      "Epoch  843/1000 Cost: 4.024611\n",
      "Epoch  844/1000 Cost: 4.024377\n",
      "Epoch  845/1000 Cost: 4.024156\n",
      "Epoch  846/1000 Cost: 4.023944\n",
      "Epoch  847/1000 Cost: 4.023723\n",
      "Epoch  848/1000 Cost: 4.023468\n",
      "Epoch  849/1000 Cost: 4.023261\n",
      "Epoch  850/1000 Cost: 4.023019\n",
      "Epoch  851/1000 Cost: 4.022784\n",
      "Epoch  852/1000 Cost: 4.022544\n",
      "Epoch  853/1000 Cost: 4.022334\n",
      "Epoch  854/1000 Cost: 4.022107\n",
      "Epoch  855/1000 Cost: 4.021869\n",
      "Epoch  856/1000 Cost: 4.021653\n",
      "Epoch  857/1000 Cost: 4.021417\n",
      "Epoch  858/1000 Cost: 4.021214\n",
      "Epoch  859/1000 Cost: 4.020951\n",
      "Epoch  860/1000 Cost: 4.020735\n",
      "Epoch  861/1000 Cost: 4.020512\n",
      "Epoch  862/1000 Cost: 4.020271\n",
      "Epoch  863/1000 Cost: 4.020034\n",
      "Epoch  864/1000 Cost: 4.019825\n",
      "Epoch  865/1000 Cost: 4.019595\n",
      "Epoch  866/1000 Cost: 4.019341\n",
      "Epoch  867/1000 Cost: 4.019118\n",
      "Epoch  868/1000 Cost: 4.018874\n",
      "Epoch  869/1000 Cost: 4.018653\n",
      "Epoch  870/1000 Cost: 4.018435\n",
      "Epoch  871/1000 Cost: 4.018198\n",
      "Epoch  872/1000 Cost: 4.017957\n",
      "Epoch  873/1000 Cost: 4.017735\n",
      "Epoch  874/1000 Cost: 4.017499\n",
      "Epoch  875/1000 Cost: 4.017287\n",
      "Epoch  876/1000 Cost: 4.017030\n",
      "Epoch  877/1000 Cost: 4.016795\n",
      "Epoch  878/1000 Cost: 4.016562\n",
      "Epoch  879/1000 Cost: 4.016326\n",
      "Epoch  880/1000 Cost: 4.016118\n",
      "Epoch  881/1000 Cost: 4.015851\n",
      "Epoch  882/1000 Cost: 4.015617\n",
      "Epoch  883/1000 Cost: 4.015428\n",
      "Epoch  884/1000 Cost: 4.015179\n",
      "Epoch  885/1000 Cost: 4.014918\n",
      "Epoch  886/1000 Cost: 4.014702\n",
      "Epoch  887/1000 Cost: 4.014464\n",
      "Epoch  888/1000 Cost: 4.014210\n",
      "Epoch  889/1000 Cost: 4.013989\n",
      "Epoch  890/1000 Cost: 4.013784\n",
      "Epoch  891/1000 Cost: 4.013529\n",
      "Epoch  892/1000 Cost: 4.013271\n",
      "Epoch  893/1000 Cost: 4.013045\n",
      "Epoch  894/1000 Cost: 4.012813\n",
      "Epoch  895/1000 Cost: 4.012603\n",
      "Epoch  896/1000 Cost: 4.012347\n",
      "Epoch  897/1000 Cost: 4.012092\n",
      "Epoch  898/1000 Cost: 4.011855\n",
      "Epoch  899/1000 Cost: 4.011653\n",
      "Epoch  900/1000 Cost: 4.011423\n",
      "Epoch  901/1000 Cost: 4.011171\n",
      "Epoch  902/1000 Cost: 4.010922\n",
      "Epoch  903/1000 Cost: 4.010688\n",
      "Epoch  904/1000 Cost: 4.010477\n",
      "Epoch  905/1000 Cost: 4.010197\n",
      "Epoch  906/1000 Cost: 4.009975\n",
      "Epoch  907/1000 Cost: 4.009727\n",
      "Epoch  908/1000 Cost: 4.009506\n",
      "Epoch  909/1000 Cost: 4.009256\n",
      "Epoch  910/1000 Cost: 4.009017\n",
      "Epoch  911/1000 Cost: 4.008779\n",
      "Epoch  912/1000 Cost: 4.008555\n",
      "Epoch  913/1000 Cost: 4.008298\n",
      "Epoch  914/1000 Cost: 4.008050\n",
      "Epoch  915/1000 Cost: 4.007821\n",
      "Epoch  916/1000 Cost: 4.007587\n",
      "Epoch  917/1000 Cost: 4.007353\n",
      "Epoch  918/1000 Cost: 4.007090\n",
      "Epoch  919/1000 Cost: 4.006863\n",
      "Epoch  920/1000 Cost: 4.006613\n",
      "Epoch  921/1000 Cost: 4.006379\n",
      "Epoch  922/1000 Cost: 4.006137\n",
      "Epoch  923/1000 Cost: 4.005911\n",
      "Epoch  924/1000 Cost: 4.005661\n",
      "Epoch  925/1000 Cost: 4.005427\n",
      "Epoch  926/1000 Cost: 4.005173\n",
      "Epoch  927/1000 Cost: 4.004906\n",
      "Epoch  928/1000 Cost: 4.004697\n",
      "Epoch  929/1000 Cost: 4.004454\n",
      "Epoch  930/1000 Cost: 4.004195\n",
      "Epoch  931/1000 Cost: 4.003942\n",
      "Epoch  932/1000 Cost: 4.003728\n",
      "Epoch  933/1000 Cost: 4.003463\n",
      "Epoch  934/1000 Cost: 4.003219\n",
      "Epoch  935/1000 Cost: 4.002979\n",
      "Epoch  936/1000 Cost: 4.002726\n",
      "Epoch  937/1000 Cost: 4.002521\n",
      "Epoch  938/1000 Cost: 4.002258\n",
      "Epoch  939/1000 Cost: 4.002007\n",
      "Epoch  940/1000 Cost: 4.001768\n",
      "Epoch  941/1000 Cost: 4.001525\n",
      "Epoch  942/1000 Cost: 4.001307\n",
      "Epoch  943/1000 Cost: 4.001021\n",
      "Epoch  944/1000 Cost: 4.000793\n",
      "Epoch  945/1000 Cost: 4.000544\n",
      "Epoch  946/1000 Cost: 4.000308\n",
      "Epoch  947/1000 Cost: 4.000035\n",
      "Epoch  948/1000 Cost: 3.999831\n",
      "Epoch  949/1000 Cost: 3.999592\n",
      "Epoch  950/1000 Cost: 3.999280\n",
      "Epoch  951/1000 Cost: 3.999078\n",
      "Epoch  952/1000 Cost: 3.998851\n",
      "Epoch  953/1000 Cost: 3.998550\n",
      "Epoch  954/1000 Cost: 3.998324\n",
      "Epoch  955/1000 Cost: 3.998085\n",
      "Epoch  956/1000 Cost: 3.997832\n",
      "Epoch  957/1000 Cost: 3.997580\n",
      "Epoch  958/1000 Cost: 3.997350\n",
      "Epoch  959/1000 Cost: 3.997109\n",
      "Epoch  960/1000 Cost: 3.996849\n",
      "Epoch  961/1000 Cost: 3.996600\n",
      "Epoch  962/1000 Cost: 3.996345\n",
      "Epoch  963/1000 Cost: 3.996101\n",
      "Epoch  964/1000 Cost: 3.995841\n",
      "Epoch  965/1000 Cost: 3.995607\n",
      "Epoch  966/1000 Cost: 3.995337\n",
      "Epoch  967/1000 Cost: 3.995106\n",
      "Epoch  968/1000 Cost: 3.994835\n",
      "Epoch  969/1000 Cost: 3.994616\n",
      "Epoch  970/1000 Cost: 3.994354\n",
      "Epoch  971/1000 Cost: 3.994102\n",
      "Epoch  972/1000 Cost: 3.993859\n",
      "Epoch  973/1000 Cost: 3.993624\n",
      "Epoch  974/1000 Cost: 3.993331\n",
      "Epoch  975/1000 Cost: 3.993090\n",
      "Epoch  976/1000 Cost: 3.992826\n",
      "Epoch  977/1000 Cost: 3.992581\n",
      "Epoch  978/1000 Cost: 3.992341\n",
      "Epoch  979/1000 Cost: 3.992081\n",
      "Epoch  980/1000 Cost: 3.991823\n",
      "Epoch  981/1000 Cost: 3.991608\n",
      "Epoch  982/1000 Cost: 3.991348\n",
      "Epoch  983/1000 Cost: 3.991057\n",
      "Epoch  984/1000 Cost: 3.990836\n",
      "Epoch  985/1000 Cost: 3.990596\n",
      "Epoch  986/1000 Cost: 3.990328\n",
      "Epoch  987/1000 Cost: 3.990093\n",
      "Epoch  988/1000 Cost: 3.989806\n",
      "Epoch  989/1000 Cost: 3.989548\n",
      "Epoch  990/1000 Cost: 3.989323\n",
      "Epoch  991/1000 Cost: 3.989060\n",
      "Epoch  992/1000 Cost: 3.988836\n",
      "Epoch  993/1000 Cost: 3.988558\n",
      "Epoch  994/1000 Cost: 3.988326\n",
      "Epoch  995/1000 Cost: 3.988060\n",
      "Epoch  996/1000 Cost: 3.987774\n",
      "Epoch  997/1000 Cost: 3.987536\n",
      "Epoch  998/1000 Cost: 3.987255\n",
      "Epoch  999/1000 Cost: 3.987017\n",
      "Epoch 1000/1000 Cost: 3.986774\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for epoch in range(epochs + 1):\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train) # 순전파\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epochs % 10 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train, y_train) # 1행의 답은 152, 2행의 답은 182, 3행의 답은 180 이라고 매칭시켜주는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True) # DataLoader는 반드시 dataset이 들어가야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [tensor([[73., 80., 75.],\n",
      "        [73., 66., 70.]]), tensor([[152.],\n",
      "        [142.]])]\n",
      "1 [tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "2 [tensor([[89., 91., 90.]]), tensor([[180.]])]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    print(i, data)\n",
    "\n",
    "# batch size가 2인데 데이터가 5개 이므로 마지막 한 개는 깍두기 취급해서 버리고 간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionModel(\n",
      "  (linear1): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (linear2): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Subclass 모델\n",
    "\n",
    "class LinearRegressionModel(nn.Module): # torch.nn.Module을 상속받는 파이썬 클래스\n",
    "    def __init__(self): #\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(3, 3)\n",
    "        self.linear2 = nn.Linear(3, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "model = LinearRegressionModel()\n",
    "print(model) # model 구조를 볼 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Batch 0/3 Cost: 29488.515625\n",
      "Epoch    0/1000 Batch 1/3 Cost: 16014586.000000\n",
      "Epoch    0/1000 Batch 2/3 Cost: 21764558.000000\n",
      "Epoch   50/1000 Batch 0/3 Cost: 33742.683594\n",
      "Epoch   50/1000 Batch 1/3 Cost: 25690.494141\n",
      "Epoch   50/1000 Batch 2/3 Cost: 21707.312500\n",
      "Epoch  100/1000 Batch 0/3 Cost: 30849.933594\n",
      "Epoch  100/1000 Batch 1/3 Cost: 24540.828125\n",
      "Epoch  100/1000 Batch 2/3 Cost: 16714.289062\n",
      "Epoch  150/1000 Batch 0/3 Cost: 21475.070312\n",
      "Epoch  150/1000 Batch 1/3 Cost: 22848.898438\n",
      "Epoch  150/1000 Batch 2/3 Cost: 27135.181641\n",
      "Epoch  200/1000 Batch 0/3 Cost: 24020.783203\n",
      "Epoch  200/1000 Batch 1/3 Cost: 20697.873047\n",
      "Epoch  200/1000 Batch 2/3 Cost: 15453.290039\n",
      "Epoch  250/1000 Batch 0/3 Cost: 23620.595703\n",
      "Epoch  250/1000 Batch 1/3 Cost: 18182.208984\n",
      "Epoch  250/1000 Batch 2/3 Cost: 11519.321289\n",
      "Epoch  300/1000 Batch 0/3 Cost: 21647.310547\n",
      "Epoch  300/1000 Batch 1/3 Cost: 16468.804688\n",
      "Epoch  300/1000 Batch 2/3 Cost: 10153.723633\n",
      "Epoch  350/1000 Batch 0/3 Cost: 15483.580078\n",
      "Epoch  350/1000 Batch 1/3 Cost: 18189.955078\n",
      "Epoch  350/1000 Batch 2/3 Cost: 10888.290039\n",
      "Epoch  400/1000 Batch 0/3 Cost: 13459.587891\n",
      "Epoch  400/1000 Batch 1/3 Cost: 11857.407227\n",
      "Epoch  400/1000 Batch 2/3 Cost: 20226.898438\n",
      "Epoch  450/1000 Batch 0/3 Cost: 12190.691406\n",
      "Epoch  450/1000 Batch 1/3 Cost: 12732.927734\n",
      "Epoch  450/1000 Batch 2/3 Cost: 14535.301758\n",
      "Epoch  500/1000 Batch 0/3 Cost: 9623.138672\n",
      "Epoch  500/1000 Batch 1/3 Cost: 15816.606445\n",
      "Epoch  500/1000 Batch 2/3 Cost: 7594.036133\n",
      "Epoch  550/1000 Batch 0/3 Cost: 12660.865234\n",
      "Epoch  550/1000 Batch 1/3 Cost: 10506.301758\n",
      "Epoch  550/1000 Batch 2/3 Cost: 6704.374023\n",
      "Epoch  600/1000 Batch 0/3 Cost: 9580.032227\n",
      "Epoch  600/1000 Batch 1/3 Cost: 8474.424805\n",
      "Epoch  600/1000 Batch 2/3 Cost: 12096.886719\n",
      "Epoch  650/1000 Batch 0/3 Cost: 8205.159180\n",
      "Epoch  650/1000 Batch 1/3 Cost: 8735.279297\n",
      "Epoch  650/1000 Batch 2/3 Cost: 10095.791016\n",
      "Epoch  700/1000 Batch 0/3 Cost: 6938.941895\n",
      "Epoch  700/1000 Batch 1/3 Cost: 6787.796387\n",
      "Epoch  700/1000 Batch 2/3 Cost: 12551.460938\n",
      "Epoch  750/1000 Batch 0/3 Cost: 10493.918945\n",
      "Epoch  750/1000 Batch 1/3 Cost: 6231.702148\n",
      "Epoch  750/1000 Batch 2/3 Cost: 2882.592529\n",
      "Epoch  800/1000 Batch 0/3 Cost: 6077.360840\n",
      "Epoch  800/1000 Batch 1/3 Cost: 5069.029297\n",
      "Epoch  800/1000 Batch 2/3 Cost: 10736.084961\n",
      "Epoch  850/1000 Batch 0/3 Cost: 5504.833984\n",
      "Epoch  850/1000 Batch 1/3 Cost: 8484.680664\n",
      "Epoch  850/1000 Batch 2/3 Cost: 2094.368164\n",
      "Epoch  900/1000 Batch 0/3 Cost: 4523.407715\n",
      "Epoch  900/1000 Batch 1/3 Cost: 7836.799805\n",
      "Epoch  900/1000 Batch 2/3 Cost: 2721.028809\n",
      "Epoch  950/1000 Batch 0/3 Cost: 5064.392578\n",
      "Epoch  950/1000 Batch 1/3 Cost: 6291.228516\n",
      "Epoch  950/1000 Batch 2/3 Cost: 2379.279053\n",
      "Epoch 1000/1000 Batch 0/3 Cost: 6712.960938\n",
      "Epoch 1000/1000 Batch 1/3 Cost: 3706.647949\n",
      "Epoch 1000/1000 Batch 2/3 Cost: 2066.576904\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 5e-4)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs + 1):\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        x, y_true = batch\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "\n",
    "        # 역전파\n",
    "        if idx == 0: # epoch 마다 작동\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(epoch, epochs, idx, len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커스텀 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class CustomDataset(torch.utils.data.Dataset): \\n  def __init__(self):\\n  데이터셋의 전처리를 해주는 부분 (데이터 경로를 지정하는 부분)\\n\\n  def __len__(self):\\n  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\\n\\n  def __getitem__(self, idx): \\n  데이터셋에서 특정 1개의 샘플을 가져오는 함수'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class CustomDataset(torch.utils.data.Dataset): \n",
    "  def __init__(self):\n",
    "  데이터셋의 전처리를 해주는 부분 (데이터 경로를 지정하는 부분)\n",
    "\n",
    "  def __len__(self):\n",
    "  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "  (데이터 전체 길이가 필요함 어디였지..)\n",
    "\n",
    "  def __getitem__(self, idx): \n",
    "  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "  (인덱스 번호를 넣어주면 n 번째 데이터를 가져올 수 있음)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 상속\n",
    "class CustomDataset(Dataset): \n",
    "  def __init__(self):\n",
    "    self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "    self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "  def __len__(self): \n",
    "    return len(self.x_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴 (이중 리스트 처리 위해)\n",
    "  def __getitem__(self, idx): \n",
    "    x = torch.FloatTensor(self.x_data[idx])\n",
    "    y = torch.FloatTensor(self.y_data[idx])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x7fb290ca9910>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CustomDataset()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([73., 80., 75.]), tensor([152.]))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 24211.177734\n",
      "Epoch    0/20 Batch 2/3 Cost: 39606.726562\n",
      "Epoch    0/20 Batch 3/3 Cost: 38482.238281\n",
      "Epoch    1/20 Batch 1/3 Cost: 30612.113281\n",
      "Epoch    1/20 Batch 2/3 Cost: 34329.390625\n",
      "Epoch    1/20 Batch 3/3 Cost: 36235.042969\n",
      "Epoch    2/20 Batch 1/3 Cost: 24211.177734\n",
      "Epoch    2/20 Batch 2/3 Cost: 40730.328125\n",
      "Epoch    2/20 Batch 3/3 Cost: 36235.042969\n",
      "Epoch    3/20 Batch 1/3 Cost: 37358.640625\n",
      "Epoch    3/20 Batch 2/3 Cost: 32860.203125\n",
      "Epoch    3/20 Batch 3/3 Cost: 25680.365234\n",
      "Epoch    4/20 Batch 1/3 Cost: 39606.726562\n",
      "Epoch    4/20 Batch 2/3 Cost: 24211.177734\n",
      "Epoch    4/20 Batch 3/3 Cost: 38482.238281\n",
      "Epoch    5/20 Batch 1/3 Cost: 32860.203125\n",
      "Epoch    5/20 Batch 2/3 Cost: 32081.300781\n",
      "Epoch    5/20 Batch 3/3 Cost: 36235.042969\n",
      "Epoch    6/20 Batch 1/3 Cost: 34329.390625\n",
      "Epoch    6/20 Batch 2/3 Cost: 37358.640625\n",
      "Epoch    6/20 Batch 3/3 Cost: 22741.990234\n",
      "Epoch    7/20 Batch 1/3 Cost: 29488.515625\n",
      "Epoch    7/20 Batch 2/3 Cost: 34329.390625\n",
      "Epoch    7/20 Batch 3/3 Cost: 38482.238281\n",
      "Epoch    8/20 Batch 1/3 Cost: 32860.203125\n",
      "Epoch    8/20 Batch 2/3 Cost: 30957.703125\n",
      "Epoch    8/20 Batch 3/3 Cost: 38482.238281\n",
      "Epoch    9/20 Batch 1/3 Cost: 40730.328125\n",
      "Epoch    9/20 Batch 2/3 Cost: 24211.177734\n",
      "Epoch    9/20 Batch 3/3 Cost: 36235.042969\n",
      "Epoch   10/20 Batch 1/3 Cost: 32081.300781\n",
      "Epoch   10/20 Batch 2/3 Cost: 32860.203125\n",
      "Epoch   10/20 Batch 3/3 Cost: 36235.042969\n",
      "Epoch   11/20 Batch 1/3 Cost: 29488.515625\n",
      "Epoch   11/20 Batch 2/3 Cost: 40730.328125\n",
      "Epoch   11/20 Batch 3/3 Cost: 25680.365234\n",
      "Epoch   12/20 Batch 1/3 Cost: 37358.640625\n",
      "Epoch   12/20 Batch 2/3 Cost: 24211.177734\n",
      "Epoch   12/20 Batch 3/3 Cost: 42978.414062\n",
      "Epoch   13/20 Batch 1/3 Cost: 32860.203125\n",
      "Epoch   13/20 Batch 2/3 Cost: 32081.300781\n",
      "Epoch   13/20 Batch 3/3 Cost: 36235.042969\n",
      "Epoch   14/20 Batch 1/3 Cost: 32860.203125\n",
      "Epoch   14/20 Batch 2/3 Cost: 32081.300781\n",
      "Epoch   14/20 Batch 3/3 Cost: 36235.042969\n",
      "Epoch   15/20 Batch 1/3 Cost: 32081.300781\n",
      "Epoch   15/20 Batch 2/3 Cost: 29488.515625\n",
      "Epoch   15/20 Batch 3/3 Cost: 42978.414062\n",
      "Epoch   16/20 Batch 1/3 Cost: 34329.390625\n",
      "Epoch   16/20 Batch 2/3 Cost: 29488.515625\n",
      "Epoch   16/20 Batch 3/3 Cost: 38482.238281\n",
      "Epoch   17/20 Batch 1/3 Cost: 32081.300781\n",
      "Epoch   17/20 Batch 2/3 Cost: 29488.515625\n",
      "Epoch   17/20 Batch 3/3 Cost: 42978.414062\n",
      "Epoch   18/20 Batch 1/3 Cost: 29488.515625\n",
      "Epoch   18/20 Batch 2/3 Cost: 34329.390625\n",
      "Epoch   18/20 Batch 3/3 Cost: 38482.238281\n",
      "Epoch   19/20 Batch 1/3 Cost: 32081.300781\n",
      "Epoch   19/20 Batch 2/3 Cost: 39606.726562\n",
      "Epoch   19/20 Batch 3/3 Cost: 22741.990234\n",
      "Epoch   20/20 Batch 1/3 Cost: 30957.703125\n",
      "Epoch   20/20 Batch 2/3 Cost: 40730.328125\n",
      "Epoch   20/20 Batch 3/3 Cost: 22741.990234\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  for batch_idx, samples in enumerate(dataloader):\n",
    "\n",
    "    x_train, y_train = samples\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 계산\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aug_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
