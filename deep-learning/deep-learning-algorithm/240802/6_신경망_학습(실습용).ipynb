{"cells":[{"cell_type":"markdown","metadata":{"id":"nyjyu4FzUAVw"},"source":["# 신경망 학습"]},{"cell_type":"markdown","metadata":{"id":"VQvNez4qydhL"},"source":["## 단순한 신경망 구현 : Logic Gate"]},{"cell_type":"markdown","metadata":{"id":"-7te43hqyiiJ"},"source":["### 필요한 모듈 import"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Qf2F_YbdybBE"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn-v0_8-whitegrid')"]},{"cell_type":"markdown","metadata":{"id":"orUoPmDcymhj"},"source":["### 하이퍼 파라미터(Hyper Parameter)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"bOAmMxo0ymDF"},"outputs":[],"source":["epochs = 1000\n","lr = 0.1"]},{"cell_type":"markdown","metadata":{"id":"BjmLWgFVysnq"},"source":["### 유틸 함수들(Util Functions)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Y4OMFGrjyq1c"},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def mean_squared_error(y_pred, y_true):\n","    return np.mean(np.power(y_true - y_pred, 2))\n","\n","def cross_entropy_error(y_pred, y_true):\n","    if y_true.ndim == 1:\n","        y_true = y_true.reshape(1, -1)\n","        y_pred = y_pred.reshape(1, -1)\n","    delta = 1e-7\n","    return -np.sum(y_true * np.log(y_pred + delta))\n","\n","def cross_entropy_error_for_batch(y_pred, y_true):\n","    if y_true.ndim == 1:\n","        y_true = y_true.reshape(1, -1)\n","        y_pred = y_pred.reshape(1, -1)\n","    delta = 1e-7\n","    batch_size = y_pred.shape[0]\n","    return -np.sum(y_true * np.log(y_pred + delta)) / batch_size\n","\n","def cross_entropy_error_for_bin(y_pred, y_true):\n","    return 0.5 * np.sum(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n","\n","def softmax(a):\n","    exp_a = np.exp(a)\n","    sum_exp_a = np.sum(exp_a)\n","    y = exp_a / sum_exp_a\n","    return y\n","\n","def differential(f, x):\n","    eps = 1e-5\n","    diff_value = np.zeros_like(x)\n","\n","    for i in range(x.shape[0]):\n","      temp_val = x[i]\n","\n","      x[i] = temp_val + eps\n","      f_h1 = f(x)\n","      x[i] = temp_val - eps\n","      f_h2 = f(x)\n","\n","      diff_value[i] = (f_h1 - f_h2) / (2*eps)\n","      x[i] = temp_val\n","\n","    return diff_value\n"]},{"cell_type":"markdown","metadata":{"id":"h5Z2LTT_y3i5"},"source":["### 신경망"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"gMTjjYgdy3D8"},"outputs":[],"source":["class LogicGateNet():\n","\n","    def __init__(self):\n","        def weight_init():\n","            np.random.seed(1)\n","            weights = np.random.randn(2)\n","            bias = np.random.rand(1)\n","\n","            return weights, bias\n","\n","        self.weights, self.bias = weight_init()\n","\n","    def predict(self, x):\n","        W = self.weights.reshape(-1, 1)\n","        b = self.bias\n","\n","        y_pred = sigmoid(np.dot(x, W) + b)\n","        return y_pred\n","\n","    def loss(self, x, y_true):\n","        y_pred = self.predict(x)\n","        return cross_entropy_error_for_bin(y_pred, y_true)\n","\n","\n","    def get_gradient(self, x, t):\n","        def loss_grad(grad):\n","            return self.loss(x, t)\n","\n","        grad_W = differential(loss_grad, self.weights)\n","        grad_b = differential(loss_grad, self.bias)\n","\n","        return grad_W, grad_b"]},{"cell_type":"markdown","metadata":{"id":"wbNDoH_3zbGZ"},"source":["### AND Gate"]},{"cell_type":"markdown","metadata":{"id":"2P-ib8_RzHTh"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"rRiaACA6zGom"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, loss: 1.5631544107938489, Weights: [ 1.59589746 -0.61601809], Bias: [-0.0709192]\n","Epoch: 100, loss: 0.6857716198204008, Weights: [1.56879204 0.80386655], Bias: [-2.15965133]\n","Epoch: 200, loss: 0.49331631794492037, Weights: [2.01797503 1.71943466], Bias: [-3.08693073]\n","Epoch: 300, loss: 0.3912166952015919, Weights: [2.43232215 2.30246439], Bias: [-3.79740008]\n","Epoch: 400, loss: 0.3251726060586437, Weights: [2.79827821 2.73621772], Bias: [-4.37788962]\n","Epoch: 500, loss: 0.2782329581540044, Weights: [3.11937435 3.08729741], Bias: [-4.87028742]\n","Epoch: 600, loss: 0.24297573701516803, Weights: [3.40282534 3.38512949], Bias: [-5.29835365]\n","Epoch: 700, loss: 0.21548037473710652, Weights: [3.65539764 3.64508755], Bias: [-5.67707715]\n","Epoch: 800, loss: 0.19343364252299072, Weights: [3.8826028  3.87631196], Bias: [-6.01662862]\n","Epoch: 900, loss: 0.17536766101399365, Weights: [4.08877055 4.08477772], Bias: [-6.32427639]\n"]}],"source":["AND = LogicGateNet()\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y = np.array([[0], [0], [0], [1]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","  grad_W, grad_b = AND.get_gradient(X, Y)\n","\n","  AND.weights -= lr * grad_W\n","  AND.bias -= lr * grad_b\n","\n","  loss = AND.loss(X, Y)\n","  train_loss_list.append(loss)\n","\n","  if i % 100 == 0:\n","    print('Epoch: {}, loss: {}, Weights: {}, Bias: {}'.format(i, loss, AND.weights, AND.bias))"]},{"cell_type":"markdown","metadata":{"id":"PZoyQv_czT7R"},"source":["#### 테스트"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"-7CvWgc9zREa"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.00135483]\n"," [0.08867878]\n"," [0.08889176]\n"," [0.87496677]]\n"]}],"source":["print(AND.predict(X))"]},{"cell_type":"markdown","metadata":{"id":"HoMXNiXWzts-"},"source":["### OR Gate"]},{"cell_type":"markdown","metadata":{"id":"DZ79pc4jzw3O"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"8gnLmAyQzuoL"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, loss: 1.0809096617574379, Weights: [ 1.64589746 -0.56601809], Bias: [0.0290808]\n","Epoch: 100, loss: 0.49357719353863294, Weights: [2.46054412 1.41789123], Bias: [-0.15085606]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 200, loss: 0.33879050434362923, Weights: [2.99128337 2.40254   ], Bias: [-0.68089049]\n","Epoch: 300, loss: 0.25670690514727385, Weights: [3.45447954 3.09023403], Bias: [-1.04026933]\n","Epoch: 400, loss: 0.2058893251459369, Weights: [3.85604032 3.61331053], Bias: [-1.30835956]\n","Epoch: 500, loss: 0.1713648634217597, Weights: [4.20522361 4.03382908], Bias: [-1.52254386]\n","Epoch: 600, loss: 0.14643542957838354, Weights: [4.51155581 4.38495379], Bias: [-1.70108626]\n","Epoch: 700, loss: 0.12763268967337307, Weights: [4.78305736 4.68615553], Bias: [-1.85419212]\n","Epoch: 800, loss: 0.11297468569483719, Weights: [5.02607716 4.9497581 ], Bias: [-1.98817781]\n","Epoch: 900, loss: 0.10124619155855695, Weights: [5.24556341 5.18403507], Bias: [-2.10724704]\n"]}],"source":["OR = LogicGateNet()\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_2 = np.array([[0], [1], [1], [1]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","  grad_W, grad_b = OR.get_gradient(X, Y_2)\n","\n","  OR.weights -= lr * grad_W\n","  OR.bias -= lr * grad_b\n","\n","  loss = OR.loss(X, Y_2)\n","  train_loss_list.append(loss)\n","\n","  if i % 100 == 0:\n","    print('Epoch: {}, loss: {}, Weights: {}, Bias: {}'.format(i, loss, OR.weights, OR.bias))"]},{"cell_type":"markdown","metadata":{"id":"jWmEtX_VnLSI"},"source":["#### 테스트"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"JwPpOs3-z2vU"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.09855987]\n"," [0.9600543 ]\n"," [0.96195283]\n"," [0.9998201 ]]\n"]}],"source":["print(OR.predict(X))"]},{"cell_type":"markdown","metadata":{"id":"JEBhczCIz57Q"},"source":["### NAND Gate"]},{"cell_type":"markdown","metadata":{"id":"TzQaaHKKz8sZ"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"h463QUQRz8PS"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, loss: 5.463758354169705, Weights: [5.39537946 5.34480461], Bias: [-2.21434882]\n","Epoch: 100, loss: 2.1532135446879206, Weights: [1.51928411 1.49576982], Bias: [-0.83310049]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 200, loss: 0.9546618606691584, Weights: [-0.42866325 -0.43623796], Bias: [1.17012553]\n","Epoch: 300, loss: 0.6147920885136836, Weights: [-1.38211578 -1.38479169], Bias: [2.40668724]\n","Epoch: 400, loss: 0.462497324741328, Weights: [-2.00130814 -2.00237709], Bias: [3.27178533]\n","Epoch: 500, loss: 0.37277579222004, Weights: [-2.47052932 -2.47100536], Bias: [3.9461259]\n","Epoch: 600, loss: 0.3125054587814956, Weights: [-2.85266861 -2.85290072], Bias: [4.50272329]\n","Epoch: 700, loss: 0.2688964904433591, Weights: [-3.17655513 -3.17667712], Bias: [4.97799844]\n","Epoch: 800, loss: 0.23578536957787494, Weights: [-3.45814174 -3.45820999], Bias: [5.39311306]\n","Epoch: 900, loss: 0.20976803763330637, Weights: [-3.70736899 -3.70740922], Bias: [5.76166182]\n"]}],"source":["NAND = LogicGateNet()\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_3 = np.array([[1], [1], [1], [0]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","  grad_W, grad_b = OR.get_gradient(X, Y_3)\n","\n","  OR.weights -= lr * grad_W\n","  OR.bias -= lr * grad_b\n","\n","  loss = OR.loss(X, Y_3)\n","  train_loss_list.append(loss)\n","\n","  if i % 100 == 0:\n","    print('Epoch: {}, loss: {}, Weights: {}, Bias: {}'.format(i, loss, OR.weights, OR.bias))"]},{"cell_type":"markdown","metadata":{"id":"jR-rHaTU0Mga"},"source":["#### 테스트"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"WpzKW6sm0Ghp"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.50002859]\n"," [0.35168472]\n"," [0.83540926]\n"," [0.73354886]]\n"]}],"source":["print(NAND.predict(X))"]},{"cell_type":"markdown","metadata":{"id":"NiTWfSQ60Zl2"},"source":["### XOR Gate"]},{"cell_type":"markdown","metadata":{"id":"hmmL0VIu0bXq"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"0CGm0r1M0a9M"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 100, loss: 1.402685224545991, Weights: [ 0.47012771 -0.19931523], Bias: [-0.16097708]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 200, loss: 1.3879445622847255, Weights: [ 0.1572739  -0.03387161], Bias: [-0.07321056]\n","Epoch: 300, loss: 1.3864920300484256, Weights: [0.05525161 0.00089673], Bias: [-0.03330094]\n","Epoch: 400, loss: 1.3863236205352059, Weights: [0.02049628 0.00504503], Bias: [-0.01514784]\n","Epoch: 500, loss: 1.3862994743646915, Weights: [0.0080051  0.00361297], Bias: [-0.00689034]\n","Epoch: 600, loss: 1.3862953430687481, Weights: [0.00326661 0.00201812], Bias: [-0.00313421]\n","Epoch: 700, loss: 1.3862945581495083, Weights: [0.00137938 0.00102449], Bias: [-0.00142566]\n","Epoch: 800, loss: 1.38629440139037, Weights: [0.00059716 0.00049628], Bias: [-0.00064849]\n","Epoch: 900, loss: 1.3862943694120302, Weights: [0.00026303 0.00023435], Bias: [-0.00029498]\n","Epoch: 1000, loss: 1.3862943628323516, Weights: [0.0001172  0.00010905], Bias: [-0.00013418]\n"]}],"source":["XOR = LogicGateNet()\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_4 = np.array([[0], [1], [1], [0]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","  grad_W, grad_b = XOR.get_gradient(X, Y_4)\n","\n","  XOR.weights -= lr * grad_W\n","  XOR.bias -= lr * grad_b\n","\n","  loss = XOR.loss(X, Y_4)\n","  train_loss_list.append(loss)\n","\n","  if i % 100 == 99:\n","    print('Epoch: {}, loss: {}, Weights: {}, Bias: {}'.format(i+1, loss, XOR.weights, XOR.bias))"]},{"cell_type":"markdown","metadata":{"id":"Cy-ktElI0o5P"},"source":["#### 테스트"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"2xHq1r8brp_f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.49996646]\n"," [0.49999372]\n"," [0.49999575]\n"," [0.50002302]]\n"]}],"source":["print(XOR.predict(X))"]},{"cell_type":"markdown","metadata":{"id":"VAlq_-6E1nIq"},"source":["#### 2층 신경망으로 XOR 게이트 구현(1)\n","\n","- 얕은 신경망, Shallow Neural Network\n","\n","- 두 논리게이트(NAND, OR)를 통과하고  \n","  AND 게이트로 합쳐서 구현\n","\n","- 06 신경망 구조 참고"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"mr7nYMG20jTo"},"outputs":[],"source":["X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_5 = np.array([[0], [1], [1], [0]])\n","\n","\n","s1 = NAND.predict(X)\n","s2 = OR.predict(X)\n","X_2 = np.array([s1, s2]).T.reshape(-1, 2)"]},{"cell_type":"markdown","metadata":{"id":"nkTDx8Ah1xHY"},"source":["#### 테스트"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"LK2iD5A91yWQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.44975235]\n"," [0.21965637]\n"," [0.69008036]\n"," [0.05502337]]\n"]}],"source":["print(AND.predict(X_2))"]},{"cell_type":"markdown","metadata":{"id":"i-SK4G262Agn"},"source":["#### 2층 신경망으로 XOR 게이트 구현(2)\n","- 클래스로 구현"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"8RpnHCRZ1zwr"},"outputs":[],"source":["class XORNet():\n","\n","  def __init__(self):\n","      np.random.seed(1)\n","\n","      def weight_init():\n","         params = {}\n","         params['W1'] = np.random.randn(2)\n","         params['b1'] = np.random.rand(2)\n","         params['W2'] = np.random.randn(2)\n","         params['b2'] = np.random.rand(1)\n","         return params\n","\n","      self.params = weight_init()\n","\n","  def predict(self, x):\n","      W1, W2 = self.params['W1'].reshape(-1, 1), self.params['W2'].reshape(-1, 1)\n","      b1, b2 = self.params['b1'], self.params['b2']\n","\n","      A1 = np.dot(x, W1) + b1\n","      Z1 = sigmoid(A1)\n","      A2 = np.dot(Z1, W2) + b2\n","      y = softmax(A2)\n","\n","      return y\n","\n","  def loss(self, x, y_true):\n","      y_pred = self.predict(x)\n","      return cross_entropy_error_for_bin(y_pred, y_true)\n","\n","  def get_gradient(self, x, t):\n","      def loss_grad(grad):\n","          return self.loss(x, t)\n","\n","      grad = {}\n","      grad['W1'] = differential(loss_grad, self.params['W1'])\n","      grad['b1'] = differential(loss_grad, self.params['b1'])\n","      grad['W2'] = differential(loss_grad, self.params['W2'])\n","      grad['b2'] = differential(loss_grad, self.params['b2'])\n","\n","      return grad"]},{"cell_type":"markdown","metadata":{"id":"lplK_x0l2YLh"},"source":["#### 하이퍼 파라미터(Hyper Parameter)\n","- 재조정"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"qf-3wWSv2b7l"},"outputs":[],"source":["lr = 0.3"]},{"cell_type":"markdown","metadata":{"id":"lmHKd45d2JbJ"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"cQNd3XVd2Gj7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 100, loss: 1.6026240994079644\n","Epoch: 200, loss: 1.448907032064322\n","Epoch: 300, loss: 0.8445558940467649\n","Epoch: 400, loss: 0.7527953318505157\n","Epoch: 500, loss: 0.7284934937785015\n","Epoch: 600, loss: 0.7178790373428695\n","Epoch: 700, loss: 0.7120378624035221\n","Epoch: 800, loss: 0.7083724381078522\n","Epoch: 900, loss: 0.7058697333055145\n","Epoch: 1000, loss: 0.7040575088733916\n"]}],"source":["XOR = XORNet()\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_5 = np.array([[0], [1], [1], [0]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","  grad = XOR.get_gradient(X, Y_5)\n","\n","  for key in ('W1', 'b1', 'W2', 'b2'):\n","    XOR.params[key] -= lr * grad[key]\n","\n","  loss = XOR.loss(X, Y_5)\n","  # print(loss)\n","  train_loss_list.append(loss)\n","\n","  if i % 100 == 99:\n","    print('Epoch: {}, loss: {}'.format(i+1, loss))"]},{"cell_type":"markdown","metadata":{"id":"IIV_GsoG2eDs"},"source":["#### 테스트"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"Dpr0nZhc2Szr"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.00363952]\n"," [0.49643508]\n"," [0.49631338]\n"," [0.00361202]]\n"]}],"source":["print(XOR.predict(X))"]},{"cell_type":"markdown","metadata":{"id":"_1IuDL8R7wrx"},"source":["## 다중 클래스 분류 : MNIST Dataset"]},{"cell_type":"markdown","metadata":{"id":"9CiJ5Gmq9Wpa"},"source":["### 배치 처리\n","- 학습 데이터 전체를 한번에 진행하지 않고  \n","  일부 데이터(샘플)을 확률적으로 구해서 조금씩 나누어 진행\n","\n","- 확률적 경사 하강법(Stochastic Gradient Descent) 또는  \n","  미니 배치 학습법(mini-batch learning)이라고도 부름"]},{"cell_type":"markdown","metadata":{"id":"YUDNWwj49byH"},"source":["#### 신경망 구현 : MNIST"]},{"cell_type":"markdown","metadata":{"id":"WjBRQYlP74GM"},"source":["#### 필요한 모듈 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h0lJbkuW71lm"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from keras.datasets import mnist\n","import time\n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","metadata":{"id":"MDvtEiD77_gu"},"source":["#### 데이터 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WL7zXMl_uo9"},"outputs":[],"source":["(x_train, y_train), (x_test, y_test) = mnist.load_data()"]},{"cell_type":"markdown","metadata":{"id":"e_rNg5Jn8FRA"},"source":["#### 데이터 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4wpsQGA8BOO"},"outputs":[],"source":["print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pU7nvkHO8IFR"},"outputs":[],"source":["img = x_train[0]\n","print(img.shape)\n","print(img.min(), img.max())\n","plt.imshow(img, cmap='gray')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbBA1Kl18KGT"},"outputs":[],"source":["label = y_train[0]\n","print(label)"]},{"cell_type":"markdown","metadata":{"id":"MTFu8i-z8U_C"},"source":["#### 데이터 전처리 (Data Preprocessing)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q76pjKDVftHJ"},"outputs":[],"source":["def flatten_for_mnist(x):\n","    temp = np.zeros((x.shape[0], x[0].size))\n","\n","    for idx, data in enumerate(x):\n","        temp[idx ,:] = data.flatten()\n","\n","    return temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vvMWrDOR8Mns"},"outputs":[],"source":["x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train = flatten_for_mnist(x_train)\n","x_test = flatten_for_mnist(x_test)\n","\n","print(x_train.shape)\n","print(x_test.shape)\n","\n","y_train_ohe = tf.one_hot(y_train, depth=10).numpy()\n","y_test_ohe = tf.one_hot(y_test, depth=10).numpy()\n","\n","print(y_train_ohe.shape)\n","print(y_test_ohe.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LjpWz0dotJs"},"outputs":[],"source":["print(x_train[0].max(), x_train[0].min())\n","print(y_train_ohe[0])"]},{"cell_type":"markdown","metadata":{"id":"5GUaa92Y9RhY"},"source":["#### 하이퍼 파라미터(Hyper Parameter)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sk3FXXLi9Th5"},"outputs":[],"source":["epochs = 2\n","lr = 0.1\n","batch_size = 100\n","train_size = x_train.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"5lMJ0h8p8iZl"},"source":["#### 사용되는 함수들(Util Functions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSlqZ2Xx8hFn"},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def mean_squared_error(y_pred, y_true):\n","    return np.mean(np.power(y_true - y_pred, 2))\n","\n","def cross_entropy_error(y_pred, y_true):\n","    if y_true.ndim == 1:\n","        y_true = y_true.reshape(1, -1)\n","        y_pred = y_pred.reshape(1, -1)\n","    delta = 1e-7\n","    return -np.sum(y_true * np.log(y_pred + delta))\n","\n","def cross_entropy_error_for_batch(y_pred, y_true):\n","    if y_true.ndim == 1:\n","        y_true = y_true.reshape(1, -1)\n","        y_pred = y_pred.reshape(1, -1)\n","    delta = 1e-7\n","    batch_size = y_pred.shape[0]\n","    return -np.sum(y_true * np.log(y_pred + delta)) / batch_size\n","\n","def cross_entropy_error_for_bin(y_pred, y_true):\n","    return 0.5 * np.sum(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n","\n","def softmax(a):\n","    exp_a = np.exp(a)\n","    sum_exp_a = np.sum(exp_a)\n","    y = exp_a / sum_exp_a\n","    return y\n","\n","def differential_1d(f, x):\n","    eps = 1e-5\n","    diff_value = np.zeros_like(x)\n","\n","    for i in range(x.shape[0]):\n","      temp_val = x[i]\n","\n","      x[i] = temp_val + eps\n","      f_h1 = f(x)\n","      x[i] = temp_val - eps\n","      f_h2 = f(x)\n","\n","      diff_value[i] = (f_h1 - f_h2) / (2*eps)\n","      x[i] = temp_val\n","\n","    return diff_value\n","\n","def differential_2d(f, X):\n","    if X.ndim == 1:\n","        return differential_1d(f, X)\n","    else :\n","        grad = np.zeros_like(X)\n","\n","        for idx, x in enumerate(X):\n","            grad[idx] = differential_1d(f, x)\n","\n","        return grad\n"]},{"cell_type":"markdown","metadata":{"id":"sSoV9fyj8_u7"},"source":["#### 2층 신경망으로 구현"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XBObD5Fw89HI"},"outputs":[],"source":["class MyModel():\n","\n","  def __init__(self):\n","\n","      def weight_init(input_nodes, hidden_nodes, output_nodes):\n","         np.random.seed(777)\n","\n","\n","         params = {}\n","         params['W1'] = 0.01 * np.random.randn(input_nodes, hidden_nodes)\n","         params['b1'] = np.zeros(hidden_nodes)\n","         params['W2'] = 0.01 * np.random.randn(hidden_nodes, output_nodes)\n","         params['b2'] = np.zeros(output_nodes)\n","\n","         return params\n","\n","      self.params = weight_init(784, 64, 10)\n","\n","  def predict(self, x):\n","      W1, W2 = self.params['W1'], self.params['W2']\n","      b1, b2 = self.params['b1'], self.params['b2']\n","\n","      A1 = np.dot(x, W1) + b1\n","      Z1 = sigmoid(A1)\n","      A2 = np.dot(Z1, W2) + b2\n","      y = softmax(A2)\n","      return y\n","\n","  def loss(self, x, y_true):\n","      y_pred = self.predict(x)\n","      return cross_entropy_error_for_bin(y_pred, y_true)\n","\n","  def accuracy(self, x, y_true):\n","      y_pred = self.predict(x)\n","      y_argmax = np.argmax(y_pred, axis=1)\n","      y_true_argmax = np.argmax(y_true, axis=1)\n","\n","      accuracy = np.mean(y_argmax == y_true_argmax)\n","      return accuracy\n","\n","  def get_gradient(self, x, t):\n","      def loss_grad(grad):\n","          return self.loss(x, t)\n","\n","      grad = {}\n","      grad['W1'] = differential_2d(loss_grad, self.params['W1'])\n","      grad['b1'] = differential_2d(loss_grad, self.params['b1'])\n","      grad['W2'] = differential_2d(loss_grad, self.params['W2'])\n","      grad['b2'] = differential_2d(loss_grad, self.params['b2'])\n","\n","      return grad"]},{"cell_type":"markdown","metadata":{"id":"maKNIlK-xJ5k"},"source":["#### 모델 생성 및 학습\n","- 시간 많이 소요"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSEARgNIop8t"},"outputs":[],"source":["model = MyModel()\n","\n","train_loss_list = list()\n","train_acc_list = list()\n","test_acc_list = list()\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","start_time = time.time()\n","\n","for i in tqdm(range(epochs)):\n","\n","    batch_idx = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_idx]\n","    y_batch = y_train_ohe[batch_idx]\n","\n","    grad = model.get_gradient(x_batch, y_batch)\n","\n","    for key in grad.keys():\n","        model.params[key] -= lr * grad[key]\n","\n","    loss = model.loss(x_batch, y_batch)\n","    train_loss_list.append(loss)\n","\n","    train_accuracy = model.accuracy(x_train, y_train_ohe)\n","    test_accuracy = model.accuracy(x_test, y_test_ohe)\n","    train_acc_list.append(train_accuracy)\n","    test_acc_list.append(test_accuracy)\n","\n","    print('Epoch: {}, Train Loss: {}, Train Accuracy: {}, Test Accuracy: {}'.format(i+1, loss, train_accuracy, test_accuracy))\n","\n","end_time = time.time()\n","\n","print('총 학습 소요시간: {:.3f}s'.format(end_time - start_time))"]},{"cell_type":"markdown","metadata":{"id":"b7nL8f20x4zl"},"source":["### 모델의 결과\n","- 모델은 학습이 잘 될 수도, 잘 안될 수도 있음\n","\n","- 만약, 학습이 잘 되지 않았다면,  \n","  학습이 잘 되기 위해서 어떠한 조치를 취해야 하는가?\n","  - 다양한 학습관련 기술이 존재"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"undefined.undefined.undefined"}},"nbformat":4,"nbformat_minor":0}
