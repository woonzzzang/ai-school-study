{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9Db_VsP_m1S"
      },
      "source": [
        "## 오차역전파 (Backpropagation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vdi2PAG_3lW"
      },
      "source": [
        "### 오차역전파 알고리즘\n",
        "- 학습 데이터로 정방향(forward) 연산을 통해 손실함수 값(loss)을 구함\n",
        "\n",
        "- 각 layer별로 역전파학습을 위해 중간값을 저장\n",
        "\n",
        "- 손실함수를 학습 파라미터(가중치, 편향)로 미분하여  \n",
        "  마지막 layer로부터 앞으로 하나씩 연쇄법칙을 이용하여 미분\n",
        "  각 layer를 통과할 때마다 저장된 값을 이용\n",
        "\n",
        "- 오류(error)를 전달하면서 학습 파라미터를 조금씩 갱신\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o545ezeUuSB"
      },
      "source": [
        "### 오차역전파 학습의 특징\n",
        "- 손실함수를 통한 평가를 한 번만 하고, 연쇄법칙을 이용한 미분을 활용하기 때문에  \n",
        "  학습 소요시간이 매우 단축!\n",
        "\n",
        "- 미분을 위한 중간값을 모두 저장하기 때문에 메모리를 많이 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFoTt-ds_w8C"
      },
      "source": [
        "### 신경망 학습에 있어서 미분가능의 중요성\n",
        "- 경사하강법(Gradient Descent)에서 손실 함수(cost function)의 최소값,  \n",
        "  즉, 최적값을 찾기 위한 방법으로 미분을 활용\n",
        "\n",
        "- 미분을 통해 손실 함수의 학습 매개변수(trainable parameter)를 갱신하여  \n",
        "  모델의 가중치의 최적값을 찾는 과정\n",
        "\n",
        "![](https://i.pinimg.com/originals/5d/13/20/5d1320c7b672710834e63b95a7c1037b.png)\n",
        "\n",
        "<sub>출처: https://www.pinterest.co.kr/pin/424816177350692379/</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilYRMqLeQf6Z"
      },
      "source": [
        "### 합성함수의 미분 (연쇄법칙, chain rule)\n",
        "\n",
        "## $\\qquad \\frac{d}{dx} [f(g(x))] = f^\\prime(g(x))g^\\prime(x)$  \n",
        "\n",
        "\n",
        "- 여러 개 연속으로 사용가능  \n",
        "  ## $ \\quad \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial u} \\times \\frac{\\partial u}{\\partial m} \\times \\frac{\\partial m}{\\partial n} \\times \\ ... \\ \\frac{\\partial l}{\\partial k} \\times \\frac{\\partial k}{\\partial g} \\times \\frac{\\partial g}{\\partial x}\n",
        "  $\n",
        "- 각각에 대해 편미분 적용가능\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/1*_KMMFvRP5X9kC59brI0ykw.png)\n",
        "<sub>출처: https://www.freecodecamp.org/news/demystifying-gradient-descent-and-backpropagation-via-logistic-regression-based-image-classification-9b5526c2ed46/</sub>\n",
        "\n",
        "- **오차역전파의 직관적 이해**\n",
        "  - 학습을 진행하면서, 즉 손실함수의 최소값(minimum)을 찾아가는 과정에서 가중치 또는 편향의 변화에 따라 얼마나 영향을 받는지 알 수 있음\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NLF8CXiQkuL"
      },
      "source": [
        "#### 합성함수 미분(chain rule) 예제\n",
        "\n",
        "![](https://miro.medium.com/max/1000/1*azqHvbrNsZ8AIZ7H75tbIQ.jpeg)\n",
        "\n",
        "<sub>출처: https://medium.com/spidernitt/breaking-down-neural-networks-an-intuitive-approach-to-backpropagation-3b2ff958794c</sub>\n",
        "\n",
        "  #### $\\quad a=-1, \\ b=3, \\ c=4$,\n",
        "  #### $\\quad x = a + b, \\ y = b + c, \\ f = x * y \\ 일 때$    \n",
        "\n",
        "\n",
        "\n",
        "  ### $\\quad \\begin{matrix}\\frac{\\partial f}{\\partial x} &=& y\\ + \\ x \\ \\frac{\\partial y}{\\partial x} \\\\\n",
        "  &=& (b \\ + \\ c) \\ + \\ (a \\ +\\ b)\\ \\times \\ 0 \\\\\n",
        "  &=& 7 \\end{matrix}$\n",
        "\n",
        "  ### $\\quad \\begin{matrix}\\frac{\\partial f}{\\partial y} &=& x\\ + \\ \\frac{\\partial x}{\\partial y} \\ y \\\\\n",
        "  &=& (a \\ + \\ b) \\ + \\ 0 \\times (b \\ +\\ c) \\\\\n",
        "  &=& 2 \\end{matrix}$\n",
        "\n",
        "   <br>\n",
        "\n",
        "  ### $ \\quad \\begin{matrix} \\frac{\\partial x}{\\partial a} &=& 1 \\ + \\ a \\ \\frac{\\partial b}{\\partial a} \\\\\n",
        "  &=& 1 \\end{matrix} $\n",
        "  ### $ \\quad \\begin{matrix} \\frac{\\partial y}{\\partial c} &=& \\frac{\\partial b}{\\partial c}\\ + 1 \\\\\n",
        "   &=& 1 \\end{matrix} $\n",
        "  \n",
        "  <br>\n",
        "\n",
        "  ### $ \\quad \\begin{matrix} \\frac{\\partial f}{\\partial a} &=& \\frac{\\partial f}{\\partial x} \\times \\frac{\\partial x}{\\partial a} \\\\\n",
        "  &=& y \\times 1 \\\\\n",
        "  &=& 7 \\times 1 = 7 \\\\\n",
        "  &=& 7  \\end{matrix} $\n",
        "    \n",
        "  ### $ \\quad \\begin{matrix} \\frac{\\partial f}{\\partial b}\\\n",
        "  &=& \\frac{\\partial x}{\\partial b} \\ y \\ + \\ x \\ \\frac{\\partial y}{\\partial b}  \\\\\n",
        "  &=& 1 \\times 7 + 2 \\times 1  \\\\\n",
        "  &=& 9 \\end{matrix} $\n",
        "  \n",
        "\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PgmdJN0Qtdw"
      },
      "source": [
        "### 덧셈, 곱셈 계층의 역전파\n",
        "- 위 예제를 통해 아래 사항을 알 수 있음\n",
        "\n",
        "  #### 1. $\\quad z = x + y$ 일 때,\n",
        "  ## $\\frac{\\partial z}{\\partial x} = 1, \\frac{\\partial z}{\\partial y} = 1 $\n",
        "\n",
        "  #### 2. $\\quad t = xy$ 일 때,\n",
        "  ## $\\frac{\\partial t}{\\partial x} = y, \\frac{\\partial t}{\\partial y} = x$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hShSxvW5WMqi"
      },
      "source": [
        "class Mul():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x * y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * self.y\n",
        "        dy = dout * self.x\n",
        "\n",
        "        return dx, dy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bovOx6UQvLP"
      },
      "source": [
        "class Add():\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x + y\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * 1\n",
        "        dy = dout * 1\n",
        "        return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3RawqlUQwp0"
      },
      "source": [
        "a, b, c  = -1, 3, 4\n",
        "x = Add()\n",
        "y = Add()\n",
        "f = Mul()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soyk-HWiSnwh"
      },
      "source": [
        "x_out = x.forward(a, b)\n",
        "y_out = y.forward(b, c)\n",
        "f_out = f.forward(x_out, y_out)\n",
        "\n",
        "print(x_out, y_out, f_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhPPrFIqSpq1"
      },
      "source": [
        "dout = 1\n",
        "dx_mul, dy_mul = f.backward(dout)\n",
        "\n",
        "da_add, db_add1 = x.backward(dx_mul)\n",
        "db_add2, dc_add = y.backward(dy_mul)\n",
        "\n",
        "print(dx_mul, dy_mul)\n",
        "print(da_add)\n",
        "print(db_add1 + db_add2)\n",
        "print(dc_add)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMfl0J1uWgiY"
      },
      "source": [
        "![](https://miro.medium.com/max/2000/1*U3mVDYuvnaLhJzIFw_d5qQ.png)\n",
        "<sub>출처: https://medium.com/spidernitt/breaking-down-neural-networks-an-intuitive-approach-to-backpropagation-3b2ff958794c</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byODNRUF5fbv"
      },
      "source": [
        "### 활성화 함수(Activation)에서의 역전파"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkRurUgm5hqn"
      },
      "source": [
        "#### 시그모이드(Sigmoid) 함수\n",
        "\n",
        "![](https://media.geeksforgeeks.org/wp-content/uploads/20190911181329/Screenshot-2019-09-11-18.05.46.png)\n",
        "\n",
        "<sub>출처: https://www.geeksforgeeks.org/implement-sigmoid-function-using-numpy/</sub>\n",
        "\n",
        "- 수식\n",
        "  # $\\quad y = \\frac{1}{1 + e^{-x}} $일 때,\n",
        "\n",
        "  ## $\\quad \\begin{matrix}y' &=& (\\frac{1}{1 + e^{-x}})' \\\\\n",
        "  &=& \\frac{-1}{(1 + e^{-x})^2}\\ \\times \\ (-e^{-x}) \\\\\n",
        "  &=& \\frac{1}{1 + e^{-x}} \\ \\times \\ \\frac{e^{-x}}{1 + e^{-x}} \\\\\n",
        "  &=& \\frac{1}{1 + e^{-x}} \\ \\times \\ (1 - \\frac{1}{1 + e^{-x}}) \\\\\n",
        "  &=& y\\ (1\\ - \\ y)\n",
        "  \\end{matrix}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNWIw7ElVhLK"
      },
      "source": [
        "class Sigmoid():\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWZ247xo5mtv"
      },
      "source": [
        "#### ReLU 함수\n",
        "\n",
        "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png)\n",
        "\n",
        "<sub>출처: https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/</sub>\n",
        "\n",
        "\n",
        "- 수식  \n",
        "\n",
        "  ### $\\qquad y=\n",
        "  \\begin{cases}\n",
        "  x & (x \\ge 0)  \\\\\n",
        "  0 & (x < 0)\n",
        "  \\end{cases}$ 일 때,\n",
        "\n",
        "  <br>\n",
        "\n",
        "  ### $\\qquad \\frac{\\partial y}{\\partial x}=\n",
        "  \\begin{cases}\n",
        "  1 & (x \\ge 0)  \\\\\n",
        "  0 & (x < 0)\n",
        "  \\end{cases}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHcPmAzh5nOZ"
      },
      "source": [
        "class ReLU():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x < 0)\n",
        "        out = x.copy()\n",
        "        out[x < 0] = 0\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH_kQzedJFfw"
      },
      "source": [
        "### 행렬 연산에 대한 역전파\n",
        "\n",
        "# $\\qquad Y = X \\bullet W + B$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-SedVrpJILw"
      },
      "source": [
        "#### 순전파(forward)\n",
        "  \n",
        "  - 형상(shape)을 맞춰줘야함\n",
        "  - 앞서 봤던 곱셈, 덧셈 계층을 합친 형태"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKIO7EzSJGD1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.random.rand(3)\n",
        "W = np.random.rand(3, 2)\n",
        "B = np.random.rand(2)\n",
        "\n",
        "print(X.shape)\n",
        "print(W.shape)\n",
        "print(B.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqvl76fFJNfU"
      },
      "source": [
        "Y = np.dot(X, W) + B\n",
        "print(Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04GHZHAiJUGl"
      },
      "source": [
        "#### 역전파(1)\n",
        "\n",
        "##  $\\  Y = X \\bullet W$\n",
        "- $X :\\ \\ (2,\\ )$\n",
        "\n",
        "- $W :\\ \\ (2,\\ 3)$\n",
        "\n",
        "- $X \\bullet W :\\ \\ (3,\\ )$\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial Y} :\\ \\ (3,\\ )$\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y}\\bullet W^T ,\\ (2,\\ )$\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial W} = X^T \\bullet \\frac{\\partial L}{\\partial Y} ,\\ (2,\\ 3)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCruMHCGJSHK"
      },
      "source": [
        "X = np.random.rand(2)\n",
        "W = np.random.rand(2, 3)\n",
        "Y = np.dot(X, W)\n",
        "\n",
        "print('X\\n{}'.format(X))\n",
        "print('W\\n{}'.format(W))\n",
        "print('Y\\n{}'.format(Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEvyFXedJWC7"
      },
      "source": [
        "dL_dY = np.random.rand(3)\n",
        "dL_dX = np.dot(dL_dY, W.T)\n",
        "dL_dW = np.dot(X.reshape(-1, 1), dL_dY.reshape(1, -1))\n",
        "\n",
        "print('dL_dY\\n{}'.format(dL_dY))\n",
        "print('dL_dX\\n{}'.format(dL_dX))\n",
        "print('dL_dW\\n{}'.format(dL_dW))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuOVqySWJauS"
      },
      "source": [
        "#### 역전파(2)\n",
        "\n",
        "## $\\ (2)\\  Y = X \\bullet W + B$\n",
        "- $X, W$는 위와 동일\n",
        "\n",
        "- $B: \\ (3, )$\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial Y}, \\ (3,\\ )$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_Jb9sY8JYGB"
      },
      "source": [
        "X = np.random.rand(2)\n",
        "W = np.random.rand(2, 3)\n",
        "B = np.random.rand(3)\n",
        "Y = np.dot(X, W) + B\n",
        "\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frdvKz8oJcoZ"
      },
      "source": [
        "dL_dY = np.random.rand(3)\n",
        "dL_dX = np.dot(dL_dY, W.T)\n",
        "dL_dW = np.dot(X.reshape(-1, 1), dL_dY.reshape(1, -1))\n",
        "dL_dB = dL_dY\n",
        "\n",
        "print('dL_dY\\n{}'.format(dL_dY))\n",
        "print('dL_dX\\n{}'.format(dL_dX))\n",
        "print('dL_dW\\n{}'.format(dL_dW))\n",
        "print('dL_dB\\n{}'.format(dL_dB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FrBSUsRJgFl"
      },
      "source": [
        "#### 배치용 행렬 내적 계층\n",
        "- N개의 데이터에 대해,  \n",
        "# $\\qquad Y = X \\bullet W + B$\n",
        "\n",
        "  - $X : \\quad  (N,\\ 3)$\n",
        "\n",
        "  - $W : \\quad  (3,\\ 2)$\n",
        "\n",
        "  - $B : \\quad  (2,\\ )$\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoWx5fDnJd-2"
      },
      "source": [
        "X = np.random.rand(4, 3)\n",
        "W = np.random.rand(3, 2)\n",
        "B = np.random.rand(2)\n",
        "\n",
        "print(X.shape)\n",
        "print(W.shape)\n",
        "print(B.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqhSX54BJmPQ"
      },
      "source": [
        "print('X\\n{}'.format(X))\n",
        "print('W\\n{}'.format(W))\n",
        "print('B\\n{}'.format(B))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1IOOfhuJqF-"
      },
      "source": [
        "Y = np.dot(X, W) + B\n",
        "print('Y\\n{}'.format(Y))\n",
        "print('Y.shape:', Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOKsN2NHJrqT"
      },
      "source": [
        "dL_dY = np.random.rand(4, 2)\n",
        "dL_dX = np.dot(dL_dY, W.T)\n",
        "dL_dW = np.dot(X.T, dL_dY)\n",
        "dL_dB = np.sum(dL_dY, axis=0)\n",
        "\n",
        "print('dL_dY\\n{}'.format(dL_dY))\n",
        "print('dL_dX\\n{}'.format(dL_dX))\n",
        "print('dL_dW\\n{}'.format(dL_dW))\n",
        "print('dL_dB\\n{}'.format(dL_dB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjIYrXGaJtEz"
      },
      "source": [
        "class Layer():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.W = np.random.randn(3, 2)\n",
        "        self.b = np.random.rand(2)\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def formward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(x, self.W) + self.b\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        return dx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twG43EylJvC-"
      },
      "source": [
        "np.random.seed(111)\n",
        "\n",
        "layer = Layer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrCDxHw5Jw3S"
      },
      "source": [
        "X = np.random.rand(2, 3)\n",
        "Y = layer.formward(X)\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boKm8OIgJyJy"
      },
      "source": [
        "dout = np.random.rand(2, 2)\n",
        "dX = layer.backward(dout)\n",
        "print(dX)\n",
        "print(layer.dW)\n",
        "print(layer.db)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz-0zFPpJ3Jz"
      },
      "source": [
        "### MNIST 분류 with 역전파\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW3InPxBJ6qo"
      },
      "source": [
        "#### Modules Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKtAhkm4J2Z3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1_2_jGMKGiT"
      },
      "source": [
        "#### 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3SQq80sJ8mR"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv_pjKdxKKIy"
      },
      "source": [
        "#### 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B95lI86tKIyY"
      },
      "source": [
        "X_train, X_test = X_train.reshape(-1, 28 * 28).astype(np.float32), X_test.reshape(-1, 28 * 28).astype(np.float32)\n",
        "X_train, X_test = X_train / .255, X_test / .255\n",
        "\n",
        "y_train = np.eye(num_classes)[y_train]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar7DZIlMKLjJ"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oye4pC82KPbg"
      },
      "source": [
        "#### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFeXORJjKM7s"
      },
      "source": [
        "epochs = 1000\n",
        "learning_rate = 1e-3\n",
        "batch_size = 100\n",
        "train_size = X_train.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIdzuyRDKSm0"
      },
      "source": [
        "#### Util Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E2_fN1dKRzv"
      },
      "source": [
        "def softmax(x):\n",
        "    if x.ndim ==2 :\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x)\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y_pred, y_true):\n",
        "    return 0.5 * np.sum((y_pred - y_true) ** 2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y_pred, y_true):\n",
        "    if y_pred.ndim == 1:\n",
        "        y_pred = y_pred.reshape(1, y_pred.size)\n",
        "        y_true = y_true.reshape(1, y_true.size)\n",
        "\n",
        "    if y_true.size == y_pred.size:\n",
        "        y_true = y_true.argmax(axis=1)\n",
        "\n",
        "    batch_size = y_pred.shape[0]\n",
        "    return -np.sum(np.log(y_pred[np.arange(batch_size), y_true] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, y_true):\n",
        "    y_pred = softmax(X)\n",
        "    return cross_entropy_error(y_pred, y_true)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBvWUNh-KY2R"
      },
      "source": [
        "#### Util Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc0sLXWJs8Dq"
      },
      "source": [
        "##### ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej2m6Robs-uJ"
      },
      "source": [
        "class ReLU():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x < 0)\n",
        "        out = x.copy()\n",
        "        out[x < 0] = 0\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RucLfG0EtYcf"
      },
      "source": [
        "##### Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWoPmtpNtZ__"
      },
      "source": [
        "class Sigmoid():\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZZeNtd-tuM5"
      },
      "source": [
        "##### Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEcCZ4bFtzgq"
      },
      "source": [
        "class Layer():\n",
        "\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.origin_x_shape = None\n",
        "\n",
        "        self.dL_dW = None\n",
        "        self.dL_db = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.origin_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(x, self.W) + self.b\n",
        "        return out\n",
        "\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dL_dW = np.dot(self.x.T, dout)\n",
        "        self.dL_db = np.sum(dout, axis=0)\n",
        "        dx = dx.reshape(*self.origin_x_shape)\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Yqp7kVvOoD"
      },
      "source": [
        "#### Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RXBVEMdvP9r"
      },
      "source": [
        "class Softmax():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y_pred = None\n",
        "        self.y_true = None\n",
        "\n",
        "    def forward(self, x, y_true):\n",
        "        self.y_pred = softmax(x)\n",
        "        self.y_true = y_true\n",
        "        self.loss = cross_entropy_error(self.y_pred, self.y_true)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.y_true.shape[0]\n",
        "\n",
        "        if self.y_true.size == self.y_pred.size:\n",
        "            dx = (self.y_pred - self.y_true) / batch_size\n",
        "        else:\n",
        "            dx = self.y_pred.copy()\n",
        "            dx[np.arange(batch_size), self.y_true] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX4I-bKfuRaq"
      },
      "source": [
        "class MyModel():\n",
        "\n",
        "    def __init__(self, input_size, hidden_size_list, output_size, activation='relu'):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size_list = hidden_size_list\n",
        "        self.output_size = output_size\n",
        "        self.hidden_layer_num = len(hidden_size_list)\n",
        "        self.params = {}\n",
        "\n",
        "        self.__init_weight(activation)\n",
        "\n",
        "        activation_layer = {'sigmoid': Sigmoid, 'relu': ReLU}\n",
        "        self.layers = OrderedDict()\n",
        "\n",
        "        for idx in range(1, self.hidden_layer_num + 1):\n",
        "            self.layers['Layer' + str(idx)] = Layer(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "            self.layers['Activation' + str(idx)] = activation_layer[activation]()\n",
        "\n",
        "        idx = self.hidden_layer_num + 1\n",
        "        self.layers['Layer' + str(idx)] = Layer(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "\n",
        "        self.last_layer = Softmax()\n",
        "\n",
        "\n",
        "    def __init_weight(self, activation):\n",
        "        weight_std = None\n",
        "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
        "\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            if activation.lower() == 'relu':\n",
        "                weight_std = np.sqrt(2 / self.input_size)  # He\n",
        "            elif activation.lower() == 'sigmoid':\n",
        "                weight_std = np.sqrt(1 / self.input_size)  # Xavier\n",
        "\n",
        "            self.params['W' + str(idx)] = weight_std * np.random.randn(all_size_list[idx - 1], all_size_list[idx])\n",
        "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, y_true):\n",
        "        y_pred = self.predict(x)\n",
        "        return self.last_layer.forward(y_pred, y_true)\n",
        "\n",
        "\n",
        "    def accuracy(self, x, y_true):\n",
        "        y_pred = self.predict(x)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        if y_true.ndim != 1:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y_pred == y_true) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "    def gradient(self, x, y_true):\n",
        "        self.loss(x, y_true)\n",
        "\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        grad = {}\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            grad['W' + str(idx)] = self.layers['Layer' + str(idx)].dL_dW\n",
        "            grad['b' + str(idx)] = self.layers['Layer' + str(idx)].dL_db\n",
        "\n",
        "        return grad\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwyNo3TsyDZR"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du5naLufxMvv"
      },
      "source": [
        "model = MyModel(28*28, [100, 64, 32], 10, activation='relu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb0HhW9x0mrN"
      },
      "source": [
        "train_loss_list = list()\n",
        "train_acc_list = list()\n",
        "test_acc_list = list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV5PciwAxM6s"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = X_train[batch_mask]\n",
        "    y_batch = y_train[batch_mask]\n",
        "\n",
        "    grad = model.gradient(x_batch, y_batch)\n",
        "\n",
        "    for key in model.params.keys():\n",
        "        model.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = model.loss(x_batch, y_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        train_acc = model.accuracy(x_batch, y_batch)\n",
        "        test_acc = model.accuracy(X_test, y_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print('Epoch: {}, Train_acc: {:.4f}, Test_acc: {:.4f}'.format(epoch, train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqqlZOF6KjDl"
      },
      "source": [
        "plt.plot(np.arange(1000//50), train_acc_list, 'r--', label='train_acc')\n",
        "plt.plot(np.arange(1000//50), test_acc_list, 'b--', label='test_acc')\n",
        "\n",
        "plt.title('Result')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc=5)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(1000), train_loss_list, 'green', label='train_loss')\n",
        "\n",
        "plt.title('Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=5)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mZqmMij6GL2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}