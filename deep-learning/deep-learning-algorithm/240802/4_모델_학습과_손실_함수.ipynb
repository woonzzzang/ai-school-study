{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn4GLsDLL1rx"
      },
      "source": [
        "# 모델 학습과 손실 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRF41Z8a-gbL"
      },
      "source": [
        "## 모델의 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 적당한 파라미터들의 꾸러미 = 모델\n",
        "# 벡터들이 어마무시하게 나오는 구조 = 모델\n",
        "# 모델의 크기 = 파라미터 수\n",
        "#  - 최소의 파라미터 수로 최대의 성능을 내는 것이 딥러닝 연구 목적"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMg1IeqAL5fi"
      },
      "source": [
        "### 지도 학습 vs 비지도 학습\n",
        "\n",
        "- 지도 학습 (Supervised Learning)\n",
        "  - 입력에 대한 정답 (Label, Ground Truth)이 존재  \n",
        "  - [입력 - 정답] 관계를 학습하여 새로운 입력에 대해 정답을 맞추는 과정\n",
        "\n",
        "- 비지도 학습 (Unsupervised Learning)\n",
        "  - 정답이 없음\n",
        "  - 데이터로부터 어떤 알고리즘을 통해 유용한 정보를 추출  \n",
        "\n",
        "![](https://www.researchgate.net/publication/329533120/figure/fig1/AS:702267594399761@1544445050584/Supervised-learning-and-unsupervised-learning-Supervised-learning-uses-annotation.png)\n",
        "<br /><sub>출처: https://www.researchgate.net/figure/Supervised-learning-and-unsupervised-learning-Supervised-learning-uses-annotation_fig1_329533120</sub>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFRSvHVq-kL1"
      },
      "source": [
        "\n",
        "## 학습 매개변수(Trainable Parameter)\n",
        "- 학습 시, 값이 변화하는 매개변수  \n",
        "  이 매개변수에 따라 학습 알고리즘(모델)이 변함\n",
        "\n",
        "- 입력에 따른 출력을 나타내는 수식, 이를 학습 모델이라고 칭함  \n",
        "  선형회귀(Linear Regression)을 예로 들면,\n",
        "\n",
        "### $\\qquad \\quad Y = aX + b $  \n",
        "\n",
        "  - $X$ : 입력\n",
        "  - $Y$ : 출력\n",
        "  - $a, b$ : 학습 매개변수\n",
        "\n",
        "- 초기화된 모델(Left Image)로부터 학습이 진행되면서  \n",
        "  학습 데이터에 맞는 모델(Right Image)로 학습 파라미터(Trainable Parameters)를 수정해 나가는 과정\n",
        "  \n",
        "![](https://learningstatisticswithr.com/book/lsr_files/figure-html/regression1b-1.png)\n",
        "\n",
        "![](https://learningstatisticswithr.com/book/lsr_files/figure-html/regression1a-1.png)\n",
        "\n",
        "<sub>출처: https://learningstatisticswithr.com/book/regression.html</sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 임의의 a, b 값을 넣은 선을 하나 넣고 데이터를 배치 단위로 넣어보면서 피팅이 되게 수정한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj3ETZY2W73s"
      },
      "source": [
        "## 하이퍼 파라미터(Hyper Parameter)\n",
        "- 사람이 직접 설정해야하는 매개변수\n",
        "- 학습이 되기전 미리 설정되어 상수취급\n",
        "  - 손실 함수 (Cost Function)\n",
        "  - 학습률 (Learning Rate)\n",
        "  - 학습 반복 횟수 (Epochs)\n",
        "  - 미니 배치 크기 (Batch Size)\n",
        "  - 은닉층의 노드 개수 (Units)\n",
        "  - 노이즈 (Noise)\n",
        "  - 규제화 (Regularization)\n",
        "  - 가중치 초기화 (Weights Initialization)\n",
        "\n",
        "- 신경망의 매개변수인 가중치는 학습 알고리즘에 의해 **자동**으로 갱신"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhTSAdjx-n_z"
      },
      "source": [
        "## 손실함수 (Loss Function, Cost Function)\n",
        "\n",
        "- 학습이 진행되면서 해당 과정이 얼마나 잘 되고 있는지 나타내는 지표\n",
        "- 손실 함수에 따른 결과를 통해 학습 파라미터를 조정\n",
        "- 최적화 이론에서 최소화 하고자 하는 함수\n",
        "- <u>**미분 가능한 함수 사용**</u>\n",
        "\n",
        "![](https://drive.google.com/uc?id=1SMxO-SrTzm8-Fq07T_rdkLnukxgPlmuL)\n",
        "<br /><sub>출처: https://zhuanlan.zhihu.com/p/85540935\\</sub>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5NFRTTjSyeE"
      },
      "source": [
        "### **학습**의 수학적 의미  \n",
        "\n",
        "![](https://s3.amazonaws.com/images.internalpointers.com/2017/10/non-convex-function.svg)\n",
        "<br /><sub>출처: https://www.internalpointers.com/post/cost-function-logistic-regression</sub>\n",
        "\n",
        "  ### $\\qquad \\qquad \\tilde{\\theta} = arg \\underset{\\theta} min \\ L(x, y\\ ; \\theta) $\n",
        "\n",
        "  - L : 손실 함수\n",
        "  - x : 학습에 사용되는 데이터의 입력값\n",
        "  - y : 학습에 사용되는 데이터의 정답\n",
        "  - $\\theta$ : 학습될 모든 파라미터(parameter)를 모은 벡터\n",
        "  - $\\tilde{\\theta}$ : 추정된 최적의 파라미터\n",
        "\n",
        "  - 학습에 사용되는 파라미터(parameter, $\\ i.g \\ $가중치($weights$), 편향($bias$), $\\ ...$ )를 모두 통칭해서 $\\theta$로 표현 가능  \n",
        "  이러한 $\\theta$의 최적값을 찾는 것이 학습\n",
        "\n",
        "  - <u>학습 데이터 입력 x와 $\\theta$에 따라 나온 예측값(predicted value)이 정답 y(label)와 비교하여 $\\theta$를 조절</u>해나가는 과정\n",
        "\n",
        "![](https://miro.medium.com/max/790/0*i6BmKLtTE5t89kJX)\n",
        "<br /><sub>출처: https://medium.com/@dhartidhami/machine-learning-basics-model-cost-function-and-gradient-descent-79b69ff28091</sub>\n",
        "\n",
        "  - 즉, **최적의 $\\theta$값에 따라 손실 함수의 가장 최저점(최솟값)을 찾는 과정**\n",
        "\n",
        "  - 손실 함수는 지도 학습(supervised learning) 알고리즘에서 반드시 필요!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XswAi5IS-rqU"
      },
      "source": [
        "### 원-핫 인코딩(one-hot encoding)\n",
        "- 범주형 변수를 표현할 때 사용\n",
        "- 가변수(Dummy Variable)이라고도 함\n",
        "- 정답인 레이블을 제외하고 0으로 처리  \n",
        "\n",
        "![](https://miro.medium.com/max/1400/0*T5jaa2othYfXZX9W.)\n",
        "<sub>출처: https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179</sub>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUnXHetp-YN7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def convert_one_hot(labels, num_classes):\n",
        "    one_hot_labels = np.zeros((len(labels), num_classes))\n",
        "    for i, label in enumerate(labels):\n",
        "        one_hot_labels[i, label] = 1\n",
        "    return one_hot_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5oNlhhV_BXh"
      },
      "outputs": [],
      "source": [
        "x_label = [1, 3, 3, 4, 2, 0, 5, 3, 0]\n",
        "print(convert_one_hot(x_label, max(x_label)+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB66tQ9G_Bxr"
      },
      "outputs": [],
      "source": [
        "from  keras.utils import to_categorical\n",
        "\n",
        "x_label = np.array([3, 2, 0, 1, 1, 6, 4, 5])\n",
        "\n",
        "one_hot_label = to_categorical(x_label, num_classes=None)\n",
        "print(one_hot_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJvOXlnq_H2W"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def convert_one_hot_sklearn(class_label):\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(class_label)\n",
        "  encoded_label = encoder.transform(class_label)\n",
        "  encoded_label = encoded_label.reshape(-1, 1)\n",
        "\n",
        "  onehot_encoder = OneHotEncoder()\n",
        "  onehot_encoder.fit(encoded_label)\n",
        "  onehot_label = onehot_encoder.transform(encoded_label)\n",
        "\n",
        "  return onehot_label.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aRLNmqH_KDK"
      },
      "outputs": [],
      "source": [
        "marvel_labels = ['아이언맨', '캡틴 아메리카', '헐크', '블랙 위도우', '스파이더맨', '앤트맨']\n",
        "ohe = convert_one_hot_sklearn(marvel_labels)\n",
        "print(ohe)\n",
        "print('One Hot encoder datatype:', type(ohe))\n",
        "print('One Hot encoder shape:', ohe.shape)\n",
        "print('--------')\n",
        "\n",
        "clases = [3, 2, 1, 3, 0, 4, 5, 3, 0]\n",
        "ohe = convert_one_hot_sklearn(clases)\n",
        "print(ohe)\n",
        "print('One Hot encoder datatype:', type(ohe))\n",
        "print('One Hot encoder shape:', ohe.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_Nav8Ar_SIL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'labels': ['아이언맨', '캡틴 아메리카', '헐크', '블랙 위도우', '스파이더맨', '앤트맨']})\n",
        "ohe_df = pd.get_dummies(df['labels'], dtype=int)\n",
        "ohe_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmvead3R_Wc7"
      },
      "source": [
        "### 평균절대오차(Mean Absolute Error, MAE)\n",
        "\n",
        "- 오차가 커져도 손실함수가 일정하게 증가\n",
        "\n",
        "- 이상치(Outlier)에 강건함(Robust)\n",
        "  -  데이터에서 [입력 - 정답] 관계가 적절하지 않은 것이 있을 경우에, 좋은 추정을 하더라도 오차가 발생하는 경우가 발생\n",
        "  - 그 때, 해당 이상치에 해당하는 지점에서 손실 함수의 최소값으로 가는 정도의 영향력이 크지 않음\n",
        "\n",
        "- 중간값(Median)과 연관\n",
        "\n",
        "- 회귀 (Regression)에 많이 사용\n",
        "\n",
        "![](https://miro.medium.com/max/1152/1*8BQhdKu1nk-tAAbOR17qGg.png)\n",
        "<br /><sub>출처: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0</sub>\n",
        "\n",
        "### $ \\qquad \\qquad E = \\frac{1}{n}\\sum_{i=1}^n \\left | y_i - \\tilde{y}_i \\right |$\n",
        "\n",
        "- $y_i$ : 학습 데이터의 $i\\ $번째 정답\n",
        "\n",
        "- $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qQA3HRU_UGM"
      },
      "outputs": [],
      "source": [
        "def MAE(y, yhat):\n",
        "  return np.mean(np.abs(y - yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ32r0KP_Y3q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "y = np.array([-3, -1, -2, 1, 3, -2, 2, 5, 3, 3, -2, -1, 2])\n",
        "yhat = np.array([-3, -1, -5, 0, 3, -1, 2, 4, 3, 3, -2, -1, -1])\n",
        "x = list(range(len(y)))\n",
        "\n",
        "plt.scatter(x, y, color='b', label='True')\n",
        "plt.plot(x, yhat, color = 'r', label='Pred')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9YRJhK3_aO6"
      },
      "outputs": [],
      "source": [
        "print(MAE(y, yhat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvqJs2n0_dmY"
      },
      "source": [
        "### 평균제곱오차(Mean Squared Error, MSE)\n",
        "- 가장 많이 쓰이는 손실 함수 중 하나\n",
        "\n",
        "- 오차가 커질수록 손실함수가 빠르게 증가\n",
        "  - 정답과 예측한 값의 차이가 클수록 더 많은 페널티를 부여\n",
        "\n",
        "- 회귀 (Regression)에 쓰임\n",
        "\n",
        "![](https://miro.medium.com/max/1152/1*EqTaoCB1NmJnsRYEezSACA.png)\n",
        "<br /><sub>출처: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0</sub>\n",
        "\n",
        "### $ \\qquad \\qquad E = \\frac{1}{n}\\sum_{i=1}^n ( y_i - \\tilde{y}_i)^2 $\n",
        "\n",
        "  - $y_i$ : 학습 데이터의 $i\\ $번째 정답\n",
        "\n",
        "  - $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-A6nvKS_b-P"
      },
      "outputs": [],
      "source": [
        "def MSE(y, yhat):\n",
        "  return  0.5 * np.sum(np.square(y - yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03jVdGED_gCm"
      },
      "outputs": [],
      "source": [
        "y = np.array([-3, -1, -2, 1, 3, -2, 2, 5, 3, 3, -2, -1, 2])\n",
        "yhat = np.array([-3, -1, -5, 0, 3, -1, 2, 4, 3, 3, -2, -1, -1])\n",
        "x = list(range(len(y)))\n",
        "\n",
        "plt.scatter(x, y, color='b', label='True')\n",
        "plt.plot(x, yhat, color = 'r', label='Pred')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9z7sBrg_hW-"
      },
      "outputs": [],
      "source": [
        "print(MSE(y, yhat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsP3yptAERwv"
      },
      "source": [
        "### 손실함수로서의 MAE와 MSE 비교\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*JTC4ReFwSeAt3kvTLq1YoA.png)\n",
        "<br /><sub>출처: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YnF-SvX_km3"
      },
      "source": [
        "### 교차 엔트로피 오차(Cross Entropy Error, CEE)\n",
        "\n",
        "- 이진 분류(Binary Classification), 다중 클래스 분류(Multi Class Classification)\n",
        "\n",
        "- 소프트맥스(softmax)와 원-핫 인코딩(ont-hot encoding) 사이의 출력 간 거리를 비교\n",
        "\n",
        "- 정답인 클래스에 대해서만 오차를 계산\n",
        "  - 정답을 맞추면 오차가 0, 틀리면 그 차이가 클수록 오차가 무한히 커짐\n",
        "\n",
        "### $ \\qquad \\qquad E = - \\frac{1}{N}\\sum_{n} \\sum_{i}  y_i\\ log\\tilde{y}_i  $\n",
        "\n",
        "- $y_i$ : 학습 데이터의 $i\\ $번째 정답 (원-핫 인코딩, one-hot encoding)\n",
        "\n",
        "\n",
        "- $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력\n",
        "\n",
        "- $N$ : 전체 데이터의 개수\n",
        "\n",
        "- $i$ : 데이터 하나당 클래스 개수\n",
        "\n",
        "- $y = log(x)$는\n",
        "  - $x$가 0에 가까울 수록 $y$값은 무한히 커짐\n",
        "  \n",
        "  - 1에 가까울 수록 0에 가까워짐\n",
        "   \n",
        "![](https://mathcracker.com/wp-content/uploads/2019/07/log-graph.png)\n",
        "<br /><sub>출처: https://mathcracker.com/finding-the-log-graph</sub>\n",
        "\n",
        "- 정답 레이블($y_i$)은 원-핫 인코딩으로 정답인 인덱스에만 1이고, 나머지는 모두 0\n",
        "\n",
        "- 따라서, 위 수식은 다음과 같이 나타낼 수 있음\n",
        "\n",
        "### $ \\qquad \\qquad E = - log\\tilde{y}_i  $\n",
        "\n",
        "  - 소프트맥스를 통해 나온 신경망 출력이 0.6이라면 $\\ -log0.6 \\fallingdotseq -0.51\\ $이 되고,  \n",
        "    신경망 출력이 0.3이라면  $\\ -log0.3 \\fallingdotseq -1.2\\ $이 됨\n",
        "\n",
        "  - 정답에 가까워질수록 오차값은 작아짐\n",
        "  \n",
        "  - 학습시, **원-핫 인코딩에 의해 정답 인덱스만 살아 남아 비교하지만, 정답이 아닌 인덱스들도 학습에 영향을 미침**  \n",
        "    <u>다중 클래스 분류는 소프트맥스(softmax) 함수를 통해 전체 항들을 모두 다루기 때문</u>\n",
        "\n",
        "![](https://miro.medium.com/max/836/1*T8KWtAn8FkAcsg8RsjiZ6Q.png)\n",
        "<br /><sub>출처: https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a</sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5tHby6S_ixZ"
      },
      "outputs": [],
      "source": [
        "def CEE(y, yhat):\n",
        "  delta = 1e-7\n",
        "  return -np.sum(y * np.log(yhat + delta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO9COZv6_oHN"
      },
      "outputs": [],
      "source": [
        "y = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
        "yhat = np.array([0.01, 0.1, 0.05, 0.0, 0.1, 0.7, 0.0, 0.03, 0.01, 0.1])\n",
        "print('yhat 합:', np.sum(yhat))\n",
        "print(CEE(y, yhat))\n",
        "print('------------')\n",
        "y = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
        "yhat = np.array([0.01, 0.1, 0.05, 0.0, 0.1, 0.03, 0.0, 0.7, 0.01, 0.1])\n",
        "print('yhat 합:', np.sum(yhat))\n",
        "print(CEE(y, yhat))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31uPTxJJ_sH8"
      },
      "source": [
        "#### 이진 분류에서의 교차 크로스 엔트로피(Binary Cross Entropy, BCE)\n",
        "\n",
        "- 이진 분류 문제(Binary Classification Problem)에서도 크로스 엔트로피 오차를 손실함수로 사용 가능\n",
        "\n",
        "### $ \\qquad \\qquad E = - \\sum_{i=1}^2  y_i\\ log\\tilde{y}_i \\\\\n",
        "\\qquad \\qquad \\ \\ \\ = -y_1\\ log\\ \\tilde{y}_1 - (1 - y_1)log(1-\\ \\tilde{y}_1) $  \n",
        "$\\qquad \\qquad \\qquad ( \\because y_2 = 1 - y_1)$\n",
        "\n",
        "- $y_i$ : 학습 데이터의 $i\\ $번째 정답 (원-핫 인코딩, one-hot encoding)\n",
        "\n",
        "\n",
        "- $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력\n",
        "\n",
        "\n",
        "- 2개의 클래스를 분류하는 문제에서  \n",
        "  1번이 정답일 확률이 0.8이고, 실제로 정답이 맞다면 위 식은 다음과 같이 나타낼 수 있음\n",
        "\n",
        "### $ \\qquad \\qquad E = -y_1\\ log\\ \\tilde{y}_1 - (1 - y_1)\\ log\\ (1-\\ \\tilde{y}_1) \\\\\n",
        "\\qquad \\qquad \\ \\ \\ = -1\\ log\\ 0.8 - (1 - 1)\\ log\\ (1 - 0.8)\\\\\n",
        "\\qquad \\qquad \\ \\ \\ = -log\\ 0.8 \\\\\n",
        "\\qquad \\qquad \\ \\ \\ \\fallingdotseq -0.22\n",
        "$\n",
        "\n",
        "- 반대로, 실제로 정답이 2번이었다면, 식은 다음과 같이 나타낼 수 있음\n",
        "\n",
        "### $ \\qquad \\qquad E = -y_1\\ log\\ \\tilde{y}_1 - (1 - y_1)\\ log\\ (1-\\ \\tilde{y}_1) \\\\\n",
        "\\qquad \\qquad \\ \\ \\ = -0\\ log\\ 0.8 - (1 - 0)\\ log\\ (1 - 0.8)\\\\\n",
        "\\qquad \\qquad \\ \\ \\ = -log\\ 0.2 \\\\\n",
        "\\qquad \\qquad \\ \\ \\ \\fallingdotseq -1.61\n",
        "$  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL93_oWd_pk8"
      },
      "outputs": [],
      "source": [
        "y = np.array([0, 1])\n",
        "yhat = np.array([0.15, 0.85])\n",
        "print('yhat 합:', np.sum(yhat))\n",
        "print(CEE(y, yhat))\n",
        "print('------------')\n",
        "y = np.array([1, 0])\n",
        "yhat = np.array([0.15, 0.85])\n",
        "print('yhat 합:', np.sum(yhat))\n",
        "print(CEE(y, yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk-PWZHrfOWh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
